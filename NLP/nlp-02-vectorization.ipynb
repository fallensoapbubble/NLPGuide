{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F50G99nH112P"
   },
   "source": [
    "# Natural Language Processing Demystified | Simple Vectorization\n",
    "https://nlpdemystified.org<br>\n",
    "https://github.com/futuremojo/nlp-demystified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9x6fL6L3zsb"
   },
   "source": [
    "### spaCy upgrade and package installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88uW0zDh4BkP"
   },
   "source": [
    "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy and download a statisical language model.\n",
    "<br><br>\n",
    "**IMPORTANT**<br>\n",
    "If you're running this in the cloud rather than using a local Jupyter server on your machine, then the notebook will **timeout** after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical packages.\n",
    "<br><br>\n",
    "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
    "https://research.google.com/colaboratory/local-runtimes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… 1. Dot Product\n",
    "\n",
    "> **\"Good when term frequency matters and vector length isn't an issue.\"**\n",
    "\n",
    "### ðŸŽ¯ Why?\n",
    "\n",
    "* Dot product rewards **documents with high frequency** of query terms.\n",
    "* Longer documents are not penalized â€” which is okay in some domains (e.g., searching code files or logs).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“˜ Example:\n",
    "\n",
    "#### Vocabulary:\n",
    "\n",
    "`[apple, banana, cat, dog]`\n",
    "\n",
    "#### Query:\n",
    "\n",
    "`Q = [1, 0, 0, 1]` â†’ \"apple dog\"\n",
    "\n",
    "#### Document A:\n",
    "\n",
    "\"apple dog dog dog\"\n",
    "â†’ Word counts: apple: 1, dog: 3\n",
    "â†’ `D_A = [1, 0, 0, 3]`\n",
    "\n",
    "#### Document B:\n",
    "\n",
    "\"apple dog\"\n",
    "â†’ Word counts: apple: 1, dog: 1\n",
    "â†’ `D_B = [1, 0, 0, 1]`\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¢ Dot Product:\n",
    "\n",
    "$$\n",
    "{Q Â· D_A} = (1Ã—1) + (0Ã—0) + (0Ã—0) + (1Ã—3) = 4\n",
    "$$\n",
    "\n",
    "$$\n",
    "{Q Â· D_B} = (1Ã—1) + (0Ã—0) + (0Ã—0) + (1Ã—1) = 2\n",
    "$$\n",
    "\n",
    "âž¡ **D\\_A ranks higher** because it repeats \"dog\" more often.\n",
    "âœ… Dot product favors documents with **higher term frequency**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 2. Cosine Similarity\n",
    "\n",
    "> **\"Best for measuring *relative similarity* regardless of length.\"**\n",
    "\n",
    "### ðŸŽ¯ Why?\n",
    "\n",
    "* Cosine similarity is normalized â€” documents of different lengths are **fairly compared**.\n",
    "* Helps avoid long documents being favored just because they repeat words more.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“˜ Example (same data as above):\n",
    "\n",
    "#### Query:\n",
    "\n",
    "`Q = [1, 0, 0, 1]`\n",
    "\n",
    "#### Document A:\n",
    "\n",
    "`D_A = [1, 0, 0, 3]`\n",
    "\n",
    "#### Document B:\n",
    "\n",
    "`D_B = [1, 0, 0, 1]`\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¢ Cosine Similarity:\n",
    "\n",
    "#### Norms:\n",
    "\n",
    "* $||Q|| = \\sqrt{1^2 + 0 + 0 + 1^2} = \\sqrt{2} \\approx 1.41$\n",
    "* $||D_A|| = \\sqrt{1^2 + 0 + 0 + 3^2} = \\sqrt{10} \\approx 3.16$\n",
    "* $||D_B|| = \\sqrt{1^2 + 0 + 0 + 1^2} = \\sqrt{2} \\approx 1.41$\n",
    "\n",
    "---\n",
    "\n",
    "#### Cosine Similarity with Document A:\n",
    "\n",
    "$$\n",
    "\\cos(Q, D_A) = \\frac{4}{1.41 \\cdot 3.16} \\approx \\frac{4}{4.46} \\approx 0.896\n",
    "$$\n",
    "\n",
    "#### Cosine Similarity with Document B:\n",
    "\n",
    "$$\n",
    "\\cos(Q, D_B) = \\frac{2}{1.41 \\cdot 1.41} = \\frac{2}{2} = 1.0\n",
    "$$\n",
    "\n",
    "âž¡ Even though D\\_A had more occurrences of \"dog\", **D\\_B is more similar** directionally.\n",
    "âœ… Cosine similarity **normalizes** the effect of document length.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 3. Binary Bag of Words (Binary BoW)\n",
    "\n",
    "> **\"Useful for boolean or very sparse models.\"**\n",
    "\n",
    "### ðŸŽ¯ Why?\n",
    "\n",
    "* Ignores frequency â€” just checks **presence or absence**.\n",
    "* Best for **simple matching** or in systems like rule-based filters, email spam detection, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“˜ Example:\n",
    "\n",
    "#### Vocabulary:\n",
    "\n",
    "`[apple, banana, cat, dog]`\n",
    "\n",
    "#### Query:\n",
    "\n",
    "`Q = [1, 0, 0, 1]` â†’ \"apple dog\"\n",
    "\n",
    "#### Document C:\n",
    "\n",
    "\"apple banana banana banana\"\n",
    "â†’ Binary Vector: `[1, 1, 0, 0]`\n",
    "\n",
    "#### Document D:\n",
    "\n",
    "\"cat dog\"\n",
    "â†’ Binary Vector: `[0, 0, 1, 1]`\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¢ Binary Matching (Intersection Count):\n",
    "\n",
    "* Query & Document C overlap: **apple**\n",
    "\n",
    "  * `Q âˆ§ C = [1, 0, 0, 0] â†’ sum = 1`\n",
    "* Query & Document D overlap: **dog**\n",
    "\n",
    "  * `Q âˆ§ D = [0, 0, 0, 1] â†’ sum = 1`\n",
    "\n",
    "âž¡ Both have a **similar match score**, regardless of how many times the words appear.\n",
    "âœ… Binary BoW is best when you just care **whether** the term is in the document â€” not how much.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Summary Chart\n",
    "\n",
    "| Method            | Uses When...                                    | Counts Frequencies? | Length-Normalized? | Example Case                |\n",
    "| ----------------- | ----------------------------------------------- | ------------------- | ------------------ | --------------------------- |\n",
    "| Dot Product       | High term frequency should boost rank           | âœ… Yes               | âŒ No               | Logs, code search           |\n",
    "| Cosine Similarity | Fair comparison across varying document lengths | âœ… Yes               | âœ… Yes              | Search engines              |\n",
    "| Binary BoW        | Presence/absence is enough                      | âŒ No                | âŒ No               | Spam filters, keyword flags |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ“˜ N-grams â€“ Notes\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Definition:\n",
    "\n",
    "An **N-gram** is a **contiguous sequence of N items** (usually words or characters) from a given text or speech.\n",
    "\n",
    "* Items can be **words**, **characters**, or even **syllables**.\n",
    "* Most commonly used: **word-level N-grams**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¢ Types of N-grams:\n",
    "\n",
    "| Name        | N Value | Example (Text: \"I love NLP\")       |\n",
    "| ----------- | ------- | ---------------------------------- |\n",
    "| **Unigram** | 1       | `[\"I\", \"love\", \"NLP\"]`             |\n",
    "| **Bigram**  | 2       | `[(\"I\", \"love\"), (\"love\", \"NLP\")]` |\n",
    "| **Trigram** | 3       | `[(\"I\", \"love\", \"NLP\")]`           |\n",
    "| **4-gram**  | 4       | Not possible (only 3 words)        |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ¨ Why Use N-grams?\n",
    "\n",
    "| Use Case                         | How N-grams Help                          |\n",
    "| -------------------------------- | ----------------------------------------- |\n",
    "| Language modeling                | Predict next word based on previous words |\n",
    "| Text classification (e.g., spam) | Capture word patterns                     |\n",
    "| Information retrieval            | Match phrase queries                      |\n",
    "| Machine translation              | Preserve local context                    |\n",
    "| Plagiarism detection             | Compare overlapping sequences             |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Word vs Character N-grams:\n",
    "\n",
    "#### Word-level N-grams:\n",
    "\n",
    "* Focuses on **tokens** (words)\n",
    "* Used in translation, document classification\n",
    "\n",
    "> Example (Bigram of \"Data science is cool\"):\n",
    "> `[\"Data science\", \"science is\", \"is cool\"]`\n",
    "\n",
    "---\n",
    "\n",
    "#### Character-level N-grams:\n",
    "\n",
    "* Breaks text into sequences of **characters**\n",
    "* Useful in **spell checking**, **language detection**, and **handling typos**\n",
    "\n",
    "> Example (Trigram of \"hello\"):\n",
    "> `[\"hel\", \"ell\", \"llo\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ Challenges:\n",
    "\n",
    "* **Data sparsity**: More N â†’ more possible combinations â†’ sparse data\n",
    "* **Memory use**: Storing all combinations can be expensive\n",
    "* **Loss of long-distance context**: N-grams only capture **local dependencies**\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Smoothing Techniques (for Language Models):\n",
    "\n",
    "When some N-grams donâ€™t appear in training data:\n",
    "\n",
    "* **Add-one/Laplace smoothing**\n",
    "* **Good-Turing discounting**\n",
    "* **Kneser-Ney smoothing**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š N-gram Frequency Table Example:\n",
    "\n",
    "For sentence:\n",
    "**\"the cat sat on the mat\"**\n",
    "\n",
    "**Bigrams**:\n",
    "\n",
    "| Bigram     | Count |\n",
    "| ---------- | ----- |\n",
    "| (the, cat) | 1     |\n",
    "| (cat, sat) | 1     |\n",
    "| (sat, on)  | 1     |\n",
    "| (on, the)  | 1     |\n",
    "| (the, mat) | 1     |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Practical Uses:\n",
    "\n",
    "| Application        | N-gram Type | Purpose                            |\n",
    "| ------------------ | ----------- | ---------------------------------- |\n",
    "| Autocomplete       | Word        | Predict next likely word           |\n",
    "| Sentiment analysis | Word        | Capture common phrase patterns     |\n",
    "| OCR / handwriting  | Character   | Match probable character sequences |\n",
    "| Language detection | Character   | Detect frequent letter patterns    |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ”¹ 1. **Bag of Words (BoW)**\n",
    "\n",
    "**Definition:**\n",
    "BoW is a method to represent text as a numerical feature vector. It considers only the frequency of words in the document, ignoring grammar and word order.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Create a **vocabulary** of all unique words from a document or corpus.\n",
    "2. Count how often each word appears in each document.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 2. **Raw Frequency**\n",
    "\n",
    "**Definition:**\n",
    "Itâ€™s just the **count** of how many times a word appears in a document.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Raw Frequency}_{t,d} = f_{t,d}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $t$ is the term (word)\n",
    "* $d$ is the document\n",
    "* $f_{t,d}$ is the frequency of term $t$ in document $d$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 3. **Term Frequency (TF)**\n",
    "\n",
    "**Definition:**\n",
    "TF is the normalized version of raw frequencyâ€”how often a term appears in a document divided by the total number of words in that document.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "TF(t, d) = \\frac{f_{t,d}}{\\sum_{k} f_{k,d}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $f_{t,d}$: Frequency of term $t$ in document $d$\n",
    "* Denominator: Total number of terms in document $d$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 4. **Inverse Document Frequency (IDF)**\n",
    "\n",
    "**Definition:**\n",
    "IDF measures how important a word is across the entire corpus. Common words like â€œtheâ€, â€œisâ€, etc., are less important and get a lower IDF.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "IDF(t) = \\log\\left(\\frac{N}{df_t}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $N$: Total number of documents\n",
    "* $df_t$: Number of documents containing term $t$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 5. **TF-IDF**\n",
    "\n",
    "**Definition:**\n",
    "Combines TF and IDF to measure the importance of a word in a document **relative to the entire corpus**.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Example:\n",
    "\n",
    "### Documents:\n",
    "\n",
    "* **Doc1**: \"the cat sat on the mat\"\n",
    "* **Doc2**: \"the dog sat on the log\"\n",
    "\n",
    "#### Step 1: Vocabulary\n",
    "\n",
    "`[the, cat, sat, on, mat, dog, log]` â†’ 7 unique words\n",
    "\n",
    "#### Step 2: Raw Frequency Table\n",
    "\n",
    "| Term | Doc1 | Doc2 |\n",
    "| ---- | ---- | ---- |\n",
    "| the  | 2    | 2    |\n",
    "| cat  | 1    | 0    |\n",
    "| sat  | 1    | 1    |\n",
    "| on   | 1    | 1    |\n",
    "| mat  | 1    | 0    |\n",
    "| dog  | 0    | 1    |\n",
    "| log  | 0    | 1    |\n",
    "\n",
    "#### Step 3: TF (Doc1)\n",
    "\n",
    "Total words in Doc1 = 6\n",
    "TF (for \"cat\" in Doc1) = 1 / 6 â‰ˆ 0.167\n",
    "TF (for \"the\" in Doc1) = 2 / 6 â‰ˆ 0.333\n",
    "\n",
    "#### Step 4: IDF (for the full corpus)\n",
    "\n",
    "Number of docs = 2\n",
    "\n",
    "* \"the\" appears in 2 â†’ $\\log(2/2) = \\log(1) = 0$\n",
    "* \"cat\" appears in 1 â†’ $\\log(2/1) = \\log(2) â‰ˆ 0.693$\n",
    "* \"sat\" appears in 2 â†’ $\\log(2/2) = 0$\n",
    "* \"dog\" appears in 1 â†’ $\\log(2/1) â‰ˆ 0.693$\n",
    "* \"log\" appears in 1 â†’ $\\log(2/1) â‰ˆ 0.693$\n",
    "\n",
    "#### Step 5: TF-IDF (for Doc1)\n",
    "\n",
    "* \"cat\": TF = 0.167 Ã— IDF = 0.693 â†’ **0.116**\n",
    "* \"the\": TF = 0.333 Ã— IDF = 0 â†’ **0**\n",
    "* \"mat\": TF = 0.167 Ã— IDF = 0.693 â†’ **0.116**\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Observations:\n",
    "\n",
    "* Common words across documents like **\"the\"** and **\"sat\"** get low or zero TF-IDF â†’ they arenâ€™t very useful for distinguishing content.\n",
    "* Unique words like **\"cat\"**, **\"mat\"**, **\"dog\"**, **\"log\"** get higher scores.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 1. **Bag of Words (BoW)**\n",
    "\n",
    "### âœ… **Purpose:**\n",
    "\n",
    "To convert text into a numerical representation using **raw word counts**. Itâ€™s simple and useful for many text classification and clustering tasks.\n",
    "\n",
    "### âž• **Pros:**\n",
    "\n",
    "* **Simple and fast** to implement\n",
    "* Works well for basic NLP tasks like spam detection or topic classification\n",
    "* **Captures presence** of keywords effectively\n",
    "\n",
    "### âž– **Cons:**\n",
    "\n",
    "* Ignores **word order** (syntax/grammar)\n",
    "* **High dimensionality**: large vocabularies = big sparse vectors\n",
    "* Doesnâ€™t capture **context** or **semantics** (e.g., â€œgreatâ€ vs. â€œgoodâ€ are treated completely separately)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. **TF-IDF (Term Frequencyâ€“Inverse Document Frequency)**\n",
    "\n",
    "### âœ… **Purpose:**\n",
    "\n",
    "To weight words based on how **important** they are in a document relative to the corpus. It reduces the impact of common words and highlights more **unique terms**.\n",
    "\n",
    "### âž• **Pros:**\n",
    "\n",
    "* Reduces the importance of common but **uninformative words**\n",
    "* Simple improvement over raw frequency\n",
    "* Keeps BoWâ€™s simplicity while making word weights more **meaningful**\n",
    "\n",
    "### âž– **Cons:**\n",
    "\n",
    "* Still **ignores word order** and **context**\n",
    "* Rare words always get high scores, even if they're **irrelevant**\n",
    "* Can be computationally expensive on very large corpora\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. **n-grams** (usually combined with BoW or TF-IDF)\n",
    "\n",
    "### âœ… **Purpose:**\n",
    "\n",
    "To capture **word sequences** (e.g., bigrams = 2-word phrases like â€œnot goodâ€). Adds some context/structure to models.\n",
    "\n",
    "### âž• **Pros:**\n",
    "\n",
    "* Captures **phrases**, which gives more **context** (e.g., â€œnot badâ€ â‰  â€œbadâ€)\n",
    "* Helps in tasks where **word order matters** (e.g., sentiment analysis)\n",
    "\n",
    "### âž– **Cons:**\n",
    "\n",
    "* **Exponential increase** in feature space (bigram = more combinations)\n",
    "* More prone to **data sparsity**\n",
    "* Still doesnâ€™t truly understand **language meaning** (compared to embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary Table\n",
    "\n",
    "| Technique   | Purpose                          | Pros                                      | Cons                                       |\n",
    "| ----------- | -------------------------------- | ----------------------------------------- | ------------------------------------------ |\n",
    "| **BoW**     | Convert text into counts         | Simple, fast, good for keyword presence   | Ignores order/meaning, sparse vectors      |\n",
    "| **TF-IDF**  | Weight words by importance       | Downweights common words, better than BoW | No order/semantics, biased to rare words   |\n",
    "| **n-grams** | Capture short sequences of words | Adds context, better for certain tasks    | High dimensionality, still no deep meaning |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ When to use what?\n",
    "\n",
    "* **BoW**: Baseline models, fast keyword-based tasks\n",
    "* **TF-IDF**: When you need slightly better feature quality with minimal complexity\n",
    "* **n-grams + TF-IDF**: For tasks where **word combinations** matter, like **sentiment**, **named entity detection**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ§  What is TF-IDF Again?\n",
    "\n",
    "**TF-IDF** stands for:\n",
    "\n",
    "* **TF (Term Frequency)**: How often a word appears in a document.\n",
    "* **IDF (Inverse Document Frequency)**: How unique that word is across all documents.\n",
    "\n",
    "ðŸ‘‰ TF-IDF = TF Ã— IDF\n",
    "\n",
    "This gives higher scores to words that are:\n",
    "\n",
    "* **Important in one document**\n",
    "* **Rare in the whole corpus**\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Beginner-Friendly Example\n",
    "\n",
    "### Suppose we have 2 short documents:\n",
    "\n",
    "* **Doc1**: \"the cat sat on the mat\"\n",
    "* **Doc2**: \"the dog sat on the log\"\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Create a Vocabulary\n",
    "\n",
    "List all unique words from all documents:\n",
    "\n",
    "```\n",
    "[\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"log\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Count Term Frequency (TF)\n",
    "\n",
    "#### For Doc1: \"the cat sat on the mat\"\n",
    "\n",
    "| Word | Frequency |\n",
    "| ---- | --------- |\n",
    "| the  | 2         |\n",
    "| cat  | 1         |\n",
    "| sat  | 1         |\n",
    "| on   | 1         |\n",
    "| mat  | 1         |\n",
    "\n",
    "Total words = 6\n",
    "So, TF = frequency / 6\n",
    "\n",
    "* TF(\"the\") = 2 / 6 = 0.333\n",
    "* TF(\"cat\") = 1 / 6 = 0.167\n",
    "* TF(\"sat\") = 1 / 6 = 0.167\n",
    "* TF(\"on\") = 1 / 6 = 0.167\n",
    "* TF(\"mat\") = 1 / 6 = 0.167\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Compute Inverse Document Frequency (IDF)\n",
    "\n",
    "Total documents = 2\n",
    "\n",
    "Now count in how many documents each word appears:\n",
    "\n",
    "| Word | Appears in how many docs | IDF              |\n",
    "| ---- | ------------------------ | ---------------- |\n",
    "| the  | 2                        | log(2/2) = 0.0   |\n",
    "| cat  | 1                        | log(2/1) â‰ˆ 0.693 |\n",
    "| sat  | 2                        | log(2/2) = 0.0   |\n",
    "| on   | 2                        | log(2/2) = 0.0   |\n",
    "| mat  | 1                        | log(2/1) â‰ˆ 0.693 |\n",
    "| dog  | 1                        | log(2/1) â‰ˆ 0.693 |\n",
    "| log  | 1                        | log(2/1) â‰ˆ 0.693 |\n",
    "\n",
    "We use **log base e** (natural log), but for simplicity we can use base 10 if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Multiply TF Ã— IDF to Get TF-IDF Scores (Doc1)\n",
    "\n",
    "| Word | TF    | IDF   | TF-IDF |\n",
    "| ---- | ----- | ----- | ------ |\n",
    "| the  | 0.333 | 0.0   | 0.000  |\n",
    "| cat  | 0.167 | 0.693 | 0.116  |\n",
    "| sat  | 0.167 | 0.0   | 0.000  |\n",
    "| on   | 0.167 | 0.0   | 0.000  |\n",
    "| mat  | 0.167 | 0.693 | 0.116  |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Represent the Document as a TF-IDF Vector\n",
    "\n",
    "Now we turn the whole document into a vector:\n",
    "\n",
    "$$\n",
    "\\text{Doc1 Vector} = [0.000, 0.116, 0.000, 0.000, 0.116, 0.000, 0.000]\n",
    "$$\n",
    "\n",
    "This corresponds to:\n",
    "\n",
    "$$\n",
    "[\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"log\"]\n",
    "$$\n",
    "\n",
    "So now the document is just a **list of numbers** = a **vector**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Why is this useful?\n",
    "\n",
    "We can now:\n",
    "\n",
    "* **Compare documents** (e.g., using cosine similarity)\n",
    "* **Feed them into ML models** like Naive Bayes, SVMs\n",
    "* **Detect important words** in a doc\n",
    "\n",
    "\n",
    "\n",
    "> â€œHow do these numbers **mean anything** to a machine?â€\n",
    "\n",
    "Letâ€™s break this down **intuitively**, step by step â€” so it **clicks**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Youâ€™re asking:\n",
    "\n",
    "**â€œWhat does a TF-IDF number like `0.116` actually mean?â€**\n",
    "**And how can a model â€˜understandâ€™ a document based on these numbers?â€**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Think of TF-IDF as a way to **quantify meaning via importance**.\n",
    "\n",
    "Imagine you're summarizing 1,000 news articles. You want to tell a computer:\n",
    "\n",
    "> â€œHey, this article is mostly about **cats**, not about **logs** or **the**.â€\n",
    "\n",
    "Thatâ€™s exactly what TF-IDF helps do. It turns a document into a **profile** of what it talks about.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Intuitive Analogy: Shopping Cart ðŸ›’\n",
    "\n",
    "Letâ€™s say a document is like a shopping cart.\n",
    "The words are like **items in the cart**, and TF-IDF gives each item a **weight**.\n",
    "\n",
    "Example TF-IDF vector for a document:\n",
    "\n",
    "| Word | TF-IDF |\n",
    "| ---- | ------ |\n",
    "| cat  | 0.116  |\n",
    "| mat  | 0.116  |\n",
    "| the  | 0.000  |\n",
    "| sat  | 0.000  |\n",
    "\n",
    "Itâ€™s like telling the computer:\n",
    "\n",
    "* ðŸ± This doc is kind of about **cats** â†’ 0.116\n",
    "* ðŸ§¼ It's a little about **mats** â†’ 0.116\n",
    "* âŒ â€œtheâ€, â€œsatâ€ â†’ not meaningful â†’ 0\n",
    "\n",
    "So now we have a **numeric fingerprint** of what this doc is about.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š How These Numbers Help in Real Tasks\n",
    "\n",
    "### âœ… 1. **Compare Documents: Cosine Similarity**\n",
    "\n",
    "TF-IDF vectors let you **compare how similar two documents are** by comparing their numbers (vectors).\n",
    "\n",
    "* Similar docs â†’ vectors pointing in the **same direction**\n",
    "* Dissimilar docs â†’ vectors point in **different directions**\n",
    "\n",
    "This is what **cosine similarity** does:\n",
    "\n",
    "> It tells you: *\"How close are these two documents in meaning?\"*\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… 2. **Use in ML Models**\n",
    "\n",
    "ML models like SVM or Naive Bayes **canâ€™t read text**.\n",
    "They work with **numbers only**.\n",
    "\n",
    "TF-IDF turns documents into vectors like:\n",
    "\n",
    "```\n",
    "Doc1 â†’ [0.0, 0.116, 0.0, 0.0, 0.116, 0.0, 0.0]\n",
    "Doc2 â†’ [0.0, 0.0, 0.116, 0.0, 0.0, 0.116, 0.116]\n",
    "```\n",
    "\n",
    "Now, the model learns:\n",
    "\n",
    "* If the word \"cat\" has a high value â†’ maybe itâ€™s about animals.\n",
    "* If the word \"log\" appears â†’ maybe itâ€™s about wood or tech.\n",
    "\n",
    "Over many documents, the model finds patterns:\n",
    "**â€œWhen TF-IDF of â€˜catâ€™ is high â†’ classify as animal-related.â€**\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… 3. **Find Important Words (Keyword Extraction)**\n",
    "\n",
    "Words with the **highest TF-IDF scores** are often the **most meaningful** in a doc.\n",
    "\n",
    "Example:\n",
    "\n",
    "Doc: â€œAI is transforming how we work and live.â€\n",
    "\n",
    "Top TF-IDF words:\n",
    "\n",
    "* â€œAIâ€ â†’ 0.7\n",
    "* â€œtransformingâ€ â†’ 0.5\n",
    "* â€œworkâ€ â†’ 0.3\n",
    "\n",
    "These are your **keywords**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  So What Do the Numbers *Mean*?\n",
    "\n",
    "They mean:\n",
    "\n",
    "> \"**How important is this word in this document, compared to all documents?**\"\n",
    "\n",
    "And this **importance score** is what helps machines:\n",
    "\n",
    "* Recognize topics\n",
    "* Compare texts\n",
    "* Classify documents\n",
    "* Search effectively\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THBGyQba4Bcm"
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy==3.*\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t81VT9JboTzt"
   },
   "source": [
    "# Basic Bag-of-Words (BOW)\n",
    "\n",
    "Course module for this demo: https://www.nlpdemystified.org/course/basic-bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_EAof8njfHz"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1IVdG29wyJ7"
   },
   "source": [
    "## Plain frequency BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fwfWQDVyJpY"
   },
   "outputs": [],
   "source": [
    "# A corpus of sentences.\n",
    "corpus = [\n",
    "  \"Red Bull drops hint on F1 engine.\",\n",
    "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
    "  \"Hamilton eyes record eighth F1 title.\",\n",
    "  \"Aston Martin announces sponsor.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILvS020Zzm6F"
   },
   "source": [
    "We want to build a basic bag-of-words (BOW) representation of our corpus. Based on what you now know from the lesson, you can probably do this from scratch using dictionaries and lists (and maybe that's a good exercise). Fortunately, there are robust libraries which make it easy.\n",
    "\n",
    "We can use the scikit-learn **CountVectorizer** which takes a collection of text documents and creates a matrix of token counts:<br>\n",
    "https://scikit-learn.org/stable/index.html<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRhJPxbHwuj_"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAphZMVPBX9P"
   },
   "source": [
    "The *fit_transform* method does two things:\n",
    "1. It learns a vocabulary dictionary from the corpus.\n",
    "2. It returns a matrix where each row represents a document and each column represents a token (i.e. term).<br>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5wi4_C7BAWv"
   },
   "outputs": [],
   "source": [
    "bow = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3Bp1XNcF1FQ"
   },
   "source": [
    "We can take a look at the features and vocabulary dictionary. Notice the **CountVectorizer** took care of tokenization for us. It also removed punctuation and lower-cased everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQbqvLgVF8B7"
   },
   "outputs": [],
   "source": [
    "# View features (tokens).\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# View vocabulary dictionary.\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dmNUkZeExam"
   },
   "source": [
    "Specifically, the **CountVectorizer** generates a sparse matrix using an efficient, compressed representation. The sparse matrix object includes a number of useful methods:\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lug2-xnAExsb"
   },
   "outputs": [],
   "source": [
    "print(type(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bywJ0XnGKPQ"
   },
   "source": [
    "If we look at the raw structure, we'll see tuples where the first element represents the document, and the second element represents a token ID. It's then followed by a count of that token. So in the second document (index 1), token 8 (\"f1\") occurs twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "At6Gt4bsEx2D"
   },
   "outputs": [],
   "source": [
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv1N1Io2EyAb"
   },
   "source": [
    "Before we explore further, we want to make a few modifications.\n",
    "1. What if we want to use another tokenizer like spaCy's?\n",
    "2. Instead of frequency, what if we want to have a binary BOW?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRgIHkzUVJtk"
   },
   "source": [
    "## Binary BOW with custom tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tof1PBgqEy1D"
   },
   "source": [
    "**CountVectorizer** supports using a custom tokenizer. For every document, it will call your tokenizer and expect a list of tokens returned. We'll create a simple callback below which has spaCy tokenize and filter tokens, and then return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcCLawrWEzC7"
   },
   "outputs": [],
   "source": [
    "# As usual, we start by importing spaCy and loading a statistical model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a tokenizer callback using spaCy under the hood. Here, we tokenize\n",
    "# the passed-in text and return the tokens, filtering out punctuation.\n",
    "def spacy_tokenizer(doc):\n",
    "  return [t.text for t in nlp(doc) if not t.is_punct]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drEe1Lv_OScv"
   },
   "source": [
    "This time, we instantiate **CountVectorizer** with our custom tokenizer (*spacy_tokenizer*), turn off case-folding, and also set the *binary* parameter to *True* so we simply get 1s and 0s marking token presence rather than token frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YREyWzaA-rT"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True)\n",
    "bow = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jDKQkZUOysa"
   },
   "source": [
    "Looking at the resulting feature names and vocabulary dictionary, we can see our *spacy_tokenizer* being used. If you're not convinced, you can remove the punctuation filtering in our tokenizer and rerun the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x6RBqTGq302"
   },
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFpQbdA-R3FI"
   },
   "source": [
    "To get a dense array representation of our sparse matrix, use *toarray*.<br>\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.toarray.html#scipy.sparse.csr_matrix.toarray\n",
    "\n",
    "We can also index and slice into the sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yGr36aP9GCr"
   },
   "outputs": [],
   "source": [
    "print('A dense representation like we saw in the slides.')\n",
    "print(bow.toarray())\n",
    "print()\n",
    "print('Indexing and slicing.')\n",
    "print(bow[0])\n",
    "print()\n",
    "print(bow[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XF0NVhdEUR1r"
   },
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leI1VuDVVP4W"
   },
   "source": [
    "Writing your own cosine similarity function is straight-forward using numpy (left as an exercise). There are multiple ways to calculate it using scipy.\n",
    "<br><br>\n",
    "One way is using the **spatial** package, which is a collection of spatial algorithms and data structures. It has a method to calculate cosine *distance*. To get the cosine *similarity*, we have to substract the distance from 1.<br>\n",
    "https://docs.scipy.org/doc/scipy/reference/spatial.html<br>\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOQQ50IgXQfH"
   },
   "outputs": [],
   "source": [
    "# The cosine method expects array_like inputs, so we need to generate\n",
    "# arrays from our sparse matrix.\n",
    "doc1_vs_doc2 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[1].toarray()[0])\n",
    "doc1_vs_doc3 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[2].toarray()[0])\n",
    "doc1_vs_doc4 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[3].toarray()[0])\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "print(f\"Doc 1 vs Doc 2: {doc1_vs_doc2}\")\n",
    "print(f\"Doc 1 vs Doc 3: {doc1_vs_doc3}\")\n",
    "print(f\"Doc 1 vs Doc 4: {doc1_vs_doc4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SRDwr2gYD04"
   },
   "source": [
    "Another approach is using scikit-learn's *cosine_similarity* which computes the metric between multiple vectors. Here, we pass it our BOW and get a matrix of cosine similarities between each document.<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwwP8-jtchSI"
   },
   "outputs": [],
   "source": [
    "# cosine_similarity can take either array-likes or sparse matrices.\n",
    "print(cosine_similarity(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I96W6qDVdDnY"
   },
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3E_hN5Ddyae"
   },
   "source": [
    "**CountVectorizer** includes an *ngram_range* parameter to generate different n-grams. n_gram range is specified using a minimum and maximum range. By default, n_gram range is set to (1, 1) which generates unigrams. Setting it to (1, 2) generates both unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZooyyRleHXe"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True, ngram_range=(1,2))\n",
    "bigrams = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Number of features: {}'.format(len(vectorizer.get_feature_names_out())))\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hvtmi3negc0G"
   },
   "outputs": [],
   "source": [
    "# Setting n_gram range to (2, 2) generates only bigrams.\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True, ngram_range=(2,2))\n",
    "bigrams = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7e40ZAKhQmm"
   },
   "source": [
    "## Basic Bag-of-Words Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbdMO0bZjROn"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Create a spacy_tokenizer callback which takes a string and returns\n",
    "# a list of tokens (each token's text) with punctuation filtered out.\n",
    "#\n",
    "corpus = [\n",
    "  \"Students use their GPS-enabled cellphones to take birdview photographs of a land in order to find specific danger points such as rubbish heaps.\",\n",
    "  \"Teenagers are enthusiastic about taking aerial photograph in order to study their neighbourhood.\",\n",
    "  \"Aerial photography is a great way to identify terrestrial features that arenâ€™t visible from the ground level, such as lake contours or river paths.\",\n",
    "  \"During the early days of digital SLRs, Canon was pretty much the undisputed leader in CMOS image sensor technology.\",\n",
    "  \"Syrian President Bashar al-Assad tells the US it will 'pay the price' if it strikes against Syria.\"\n",
    "]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_tokenizer(doc):\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjBJUUpcBWp2"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Initialize a CountVectorizer object and set it to use\n",
    "# your spacy_tokenizer with lower-casing off and to create a binary BOW.\n",
    "#\n",
    "\n",
    "# Instantiate a CountVectorizer object called 'vectorizer'.\n",
    "\n",
    "\n",
    "# Create a binary BOW from the corpus using your CountVectorizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "os3tPj5nmRLw"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# The string below is a whole paragraph. We want to create another\n",
    "# binary BOW but using the vocabulary of our *current* CountVectorizer. This means\n",
    "# that words in this paragraph which AREN'T already in the vocabulary won't be\n",
    "# represented. This is to illustrate how BOW can't handle out-of-vocabulary words\n",
    "# unless you rebuild your whole vocabulary. Still, we'll see that if there's\n",
    "# enough overlapping vocabulary, some similarity can still be picked up.\n",
    "#\n",
    "# Note that we call 'transform' only instead of 'fit_transform' because the\n",
    "# fit step (i.e. vocabulary build) is already done and we don't want to re-fit here.\n",
    "#\n",
    "s = [\"Teenagers take aerial shots of their neighbourhood using digital cameras sitting in old bottles which are launched via kites - a common toy for children living in the favelas. They then use GPS-enabled smartphones to take pictures of specific danger points - such as rubbish heaps, which can become a breeding ground for mosquitoes carrying dengue fever.\"]\n",
    "new_bow = vectorizer.transform(s)\n",
    "\n",
    "#\n",
    "# EXERCISE: using the pairwise cosine_similarity method from sklearn,\n",
    "# calculate the similarities between each document from the corpus against\n",
    "# this new document (new_bow). HINT: You can pass two parameters to\n",
    "# cosine_similarity in this case. See the docs:\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine\n",
    "#\n",
    "# Which document is the most similar? Which is the least similar? Do the results make sense\n",
    "# based on what you see?\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXThYmDiwMmR"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Implement your own cosine similarity method using numpy.\n",
    "# It should take two numpy arrays and output the similarity metric.\n",
    "# HINTS:\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
    "#\n",
    "# Verify the similarity between the first document in the corpus and the\n",
    "# paragraph is the same as the one you got from using pairwise cosine_similarity.\n",
    "#\n",
    "import numpy as np\n",
    "def cos_sim(a, b):\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghlqn6l-dal4"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: In spacy_tokenizer, instead of returning the plain text,\n",
    "# return the lemma_ attribute instead. How do the cosine similarity\n",
    "# results differ? What if you filter out stop words as well?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnC_i4oH2ARW"
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "Course module for this demo: https://www.nlpdemystified.org/course/tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb7W_O_FS3H6"
   },
   "source": [
    "**NOTE: If the notebook timed out, you may need to re-upgrade spaCy and re-install the language model as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRtp9F8KS5QE"
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy==3.*\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMwv39AfP7Ti"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmcTBtSx-XqZ"
   },
   "source": [
    "## Fetching datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYkq3i7_-qhQ"
   },
   "source": [
    "This time around, rather than using a short toy corpus, let's use a larger dataset. scikit-learn has a **datasets** module with utilties to load datasets of our own as well as fetch popular reference datasets online.<br>\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\n",
    "<br><br>\n",
    "We'll use the **20 newsgroups** dataset, which is a collection of 18,000 newsgroup posts across 20 topics.<br>\n",
    "https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
    "<br><br>\n",
    "List of datasets available:<br>\n",
    "https://scikit-learn.org/stable/datasets.html#datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYjxqxVBBINV"
   },
   "source": [
    "The **datasets** module includes fetchers for each dataset in scikit-learn. For our purposes, we'll fetch only the posts from the *sci.space* topic, and skip on headers, footers, and quoting of other posts.<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\n",
    "<br><br>\n",
    "By default, the fetcher retrieves the *training* subset of the data only. If you don't know what that means, it'll become clear later in the course when we discuss modelling. For now, it doesn't matter for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9to6gQNCGiN"
   },
   "outputs": [],
   "source": [
    "corpus = fetch_20newsgroups(categories=['sci.space'],\n",
    "                            remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W989GHQxDvTW"
   },
   "source": [
    "We get back a **Bunch** container object containing the data as well as other information.<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html\n",
    "<br><br>\n",
    "The actual posts are accessed through the *data* attribute and is a list of strings, each one representing a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POGdVmdIDuCK"
   },
   "outputs": [],
   "source": [
    "print(type(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6AgmbL0ES9I"
   },
   "outputs": [],
   "source": [
    "# Number of posts in our dataset.\n",
    "len(corpus.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAjM4uNDEXGf"
   },
   "outputs": [],
   "source": [
    "# View first two posts.\n",
    "corpus.data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH99M6cxCpsz"
   },
   "source": [
    "## Creating TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtnQX-wWDhGh"
   },
   "outputs": [],
   "source": [
    "# Like before, if we want to use spaCy's tokenizer, we need\n",
    "# to create a callback. Remember to upgrade spaCy if you need\n",
    "# to (refer to beginnning of file for commentary and instructions).\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# We don't need named-entity recognition nor dependency parsing for\n",
    "# this so these components are disabled. This will speed up the\n",
    "# pipeline. We do need part-of-speech tagging however.\n",
    "unwanted_pipes = [\"ner\", \"parser\"]\n",
    "\n",
    "# For this exercise, we'll remove punctuation and spaces (which\n",
    "# includes newlines), filter for tokens consisting of alphabetic\n",
    "# characters, and return the lemma (which require POS tagging).\n",
    "def spacy_tokenizer(doc):\n",
    "  with nlp.disable_pipes(*unwanted_pipes):\n",
    "    return [t.lemma_ for t in nlp(doc) if \\\n",
    "            not t.is_punct and \\\n",
    "            not t.is_space and \\\n",
    "            t.is_alpha]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il-0gY9LEiNv"
   },
   "source": [
    "Like the classes to create raw frequency and binary bag-of-words vectors, scikit-learn includes a similar class called **TfidfVectorizer** to create TF-IDF vectors from a corpus.<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "<br><br>\n",
    "The usage pattern is similar in that we call *fit_transform* on the corpus which generates the vocabulary dictionary (fit step), and generates the TF-IDF vectors (transform step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Shj6BS0BN6FU"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Use the default settings of TfidfVectorizer.\n",
    "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
    "features = vectorizer.fit_transform(corpus.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZ9w4gh9sobB"
   },
   "outputs": [],
   "source": [
    "# The number of unique tokens.\n",
    "print(len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CxmKlPcNRLk"
   },
   "outputs": [],
   "source": [
    "# The dimensions of our feature matrix. X rows (documents) by Y columns (tokens).\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJwnU8PZNdHU"
   },
   "outputs": [],
   "source": [
    "# What the encoding of the first document looks like in sparse format.\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp7VTwYzONlt"
   },
   "source": [
    "As we mentioned in the slides, there are TF-IDF variations out there and scikit-learn, among other things, adds **smoothing** (adds a one to the numerator and denominator in the IDF component), and normalizes by default. These can be disabled if desired using the *smooth_idf* and *norm* parameters respectively. See here for more information:<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylKLM-IMOwbJ"
   },
   "source": [
    "## Querying the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8oTtCg0QB71"
   },
   "source": [
    "The similarity measuring techniques we learned previously can be used here in the same way. In effect, we can query our data using this sequence:\n",
    "1. *Transform* our query using the same vocabulary from our *fit* step on our corpus.\n",
    "2. Calculate the pairwise cosine similarities between each document in our corpus and our query.\n",
    "3. Sort them in descending order by score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNjEUzqlP6Oy"
   },
   "outputs": [],
   "source": [
    "# Transform the query into a TF-IDF vector.\n",
    "query = [\"lunar orbit\"]\n",
    "query_tfidf = vectorizer.transform(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEfdfkmpP8Tv"
   },
   "outputs": [],
   "source": [
    "# Calculate the cosine similarities between the query and each document.\n",
    "# We're calling flatten() here becaue cosine_similarity returns a list\n",
    "# of lists and we just want a single list.\n",
    "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skuSFhLxXOMC"
   },
   "source": [
    "Now that we have our list of cosine similarities, we can use this utility function to return the indices of the top k documents with the highest cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0PvqRDpUSYO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numpy's argsort() method returns a list of *indices* that\n",
    "# would sort an array:\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
    "#\n",
    "# The sort is ascending, but we want the largest k cosine_similarites\n",
    "# at the bottom of the sort. So we negate k, and get the last k\n",
    "# entries of the indices list in reverse order. There are faster\n",
    "# ways to do this using things like argpartition but this is\n",
    "# more succinct.\n",
    "def top_k(arr, k):\n",
    "  kth_largest = (k + 1) * -1\n",
    "  return np.argsort(arr)[:kth_largest:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFYpEldVUaAG"
   },
   "outputs": [],
   "source": [
    "# So for our query above, these are the top five documents.\n",
    "top_related_indices = top_k(cosine_similarities, 5)\n",
    "print(top_related_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4e86P3bQR1ZS"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at their respective cosine similarities.\n",
    "print(cosine_similarities[top_related_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzdyTptURiTQ"
   },
   "outputs": [],
   "source": [
    "# Top match.\n",
    "print(corpus.data[top_related_indices[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQwWXypfR8vh"
   },
   "outputs": [],
   "source": [
    "# Second-best match.\n",
    "print(corpus.data[top_related_indices[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-5aqUbGSM5J"
   },
   "outputs": [],
   "source": [
    "# Try a different query\n",
    "query = [\"satellite\"]\n",
    "query_tfidf = vectorizer.transform(query)\n",
    "\n",
    "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()\n",
    "top_related_indices = top_k(cosine_similarities, 5)\n",
    "\n",
    "print(top_related_indices)\n",
    "print(cosine_similarities[top_related_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHQtRQIcSbTj"
   },
   "outputs": [],
   "source": [
    "print(corpus.data[top_related_indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4v5wQ4JaBIh"
   },
   "source": [
    "So here we have the beginnings of a simple search engine but we're a far cry from competing with commercial off-the-shelf search engines, let alone Google.\n",
    "<br>\n",
    "- For each query, we're scanning through our entire corpus, but in practice, you'll want to create an **inverted index**. Search applications such as Elasticsearch do that under the hood.\n",
    "- You'd also want to evaluate the efficacy of your search using metrics like **precision** and **recall**.\n",
    "- Document ranking also tends to be more sophisticated, using different ranking functions like Okapi BM25. With major search engines, ranking also involves hundreds of variables such as what the user searched for previously, what do they tend to click on, where are they physically, and on and on. These variables are part of the \"secret sauce\" and are closely guarded by companies.\n",
    "- Beyond word presence, intent and meaning are playing a larger role.\n",
    "<br>\n",
    "\n",
    "Information Retrieval is a huge, rich topic and beyond search, it's also key in tasks such as question-answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak3LXiETfGIY"
   },
   "source": [
    "## TF-IDF Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08nTQB7_fJU0"
   },
   "source": [
    "**EXERCISE**<br>\n",
    "Read up on these concepts we just mentioned if you're curious.<br>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Inverted_index<br>\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall<br>\n",
    "https://en.wikipedia.org/wiki/Okapi_BM25<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iz2FCCq1fsjz"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: fetch multiple topics from the 20 newsgroups\n",
    "# dataset and query them using the approach we followed.\n",
    "# A list of topics can be found here:\n",
    "# https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
    "#\n",
    "# If you're feeling ambitious, incorporate n-grams or\n",
    "# look at how you can measure precision and recall.\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlpdemystified-vectorization.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

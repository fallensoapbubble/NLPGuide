{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juhZQ_vsQxKX"
   },
   "source": [
    "\n",
    "\n",
    "## üß† **What is LangChain?**\n",
    "\n",
    "**LangChain** is an open-source **framework** designed to help developers build **applications using large language models (LLMs)** like OpenAI's **GPT-4**, **Anthropic‚Äôs Claude**, or other LLMs.\n",
    "\n",
    "It makes it easier to:\n",
    "\n",
    "* **Connect LLMs to external data**\n",
    "* **Build multi-step reasoning workflows (chains)**\n",
    "* **Integrate memory, tools, and agents**\n",
    "* **Deploy LLM-powered apps** like chatbots, document analyzers, code assistants, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **Core Components of LangChain**\n",
    "\n",
    "| Component            | Description                                                                 |\n",
    "| -------------------- | --------------------------------------------------------------------------- |\n",
    "| **LLMs**             | Use models like GPT via OpenAI API, Azure, etc.                             |\n",
    "| **Prompt Templates** | Pre-defined prompts that help structure inputs to the LLM.                  |\n",
    "| **Chains**           | Sequences of calls (prompts ‚Üí LLM ‚Üí output ‚Üí next step).                    |\n",
    "| **Memory**           | Keeps track of past interactions (e.g., chat history).                      |\n",
    "| **Agents**           | Allow LLMs to make decisions and use tools like web search or code exec.    |\n",
    "| **Tools**            | External utilities (e.g., calculators, APIs, file systems, vector DBs).     |\n",
    "| **Retrievers**       | Pull relevant data from external sources like PDFs, websites, or databases. |\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ **LangChain Workflow Example**\n",
    "\n",
    "1. **User Input**\n",
    "   ‚û°Ô∏è 2. **Prompt Template**\n",
    "   ‚û°Ô∏è 3. **LLM Call (OpenAI API)**\n",
    "   ‚û°Ô∏è 4. **Tool/Agent Access (e.g., Search or DB)**\n",
    "   ‚û°Ô∏è 5. **Final Response**\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ **Typical Applications Built with LangChain**\n",
    "\n",
    "* Chatbots with memory\n",
    "* RAG (Retrieval-Augmented Generation) systems\n",
    "* Data analysis tools (code interpreters)\n",
    "* Legal, medical, or financial assistants\n",
    "* PDF/Document Q\\&A apps\n",
    "* Custom GPT-style apps using your data\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **Why Use LangChain?**\n",
    "\n",
    "* Scales LLM applications faster\n",
    "* Built-in support for memory, chaining, tool use\n",
    "* Ideal for production-ready AI assistants\n",
    "* Easy to prototype complex workflows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d4bu8QBS7R3",
    "outputId": "0094b4e0-fe52-4cad-bab1-db7d830ebeff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
      "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.69)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.176.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain  langchain-google-genai google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "PZAlmsRsT8Sv"
   },
   "outputs": [],
   "source": [
    "gkey=\"AIzaSyCOpoQvsNT6ylEd87-lY7-_b2YeiMfyaws\"\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = gkey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0jI5G7cjaM1f",
    "outputId": "2b607eec-b2ab-4ae3-e3bb-37cb5ef27272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing available Gemini models and their supported methods:\n",
      "{'description': 'Obtain a distributed representation of a text.',\n",
      " 'display_name': 'Embedding Gecko',\n",
      " 'input_token_limit': 1024,\n",
      " 'name': 'models/embedding-gecko-001',\n",
      " 'output_token_limit': 1,\n",
      " 'supported_generation_methods': ['embedText', 'countTextTokens'],\n",
      " 'temperature': None,\n",
      " 'top_k': None,\n",
      " 'top_p': None,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'The original Gemini 1.0 Pro Vision model version which was '\n",
      "                'optimized for image understanding. Gemini 1.0 Pro Vision was '\n",
      "                'deprecated on July 12, 2024. Move to a newer Gemini version.',\n",
      " 'display_name': 'Gemini 1.0 Pro Vision',\n",
      " 'input_token_limit': 12288,\n",
      " 'name': 'models/gemini-1.0-pro-vision-latest',\n",
      " 'output_token_limit': 4096,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 0.4,\n",
      " 'top_k': 32,\n",
      " 'top_p': 1.0,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'The original Gemini 1.0 Pro Vision model version which was '\n",
      "                'optimized for image understanding. Gemini 1.0 Pro Vision was '\n",
      "                'deprecated on July 12, 2024. Move to a newer Gemini version.',\n",
      " 'display_name': 'Gemini 1.0 Pro Vision',\n",
      " 'input_token_limit': 12288,\n",
      " 'name': 'models/gemini-pro-vision',\n",
      " 'output_token_limit': 4096,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 0.4,\n",
      " 'top_k': 32,\n",
      " 'top_p': 1.0,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Alias that points to the most recent production '\n",
      "                '(non-experimental) release of Gemini 1.5 Pro, our mid-size '\n",
      "                'multimodal model that supports up to 2 million tokens.',\n",
      " 'display_name': 'Gemini 1.5 Pro Latest',\n",
      " 'input_token_limit': 2000000,\n",
      " 'name': 'models/gemini-1.5-pro-latest',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Stable version of Gemini 1.5 Pro, our mid-size multimodal '\n",
      "                'model that supports up to 2 million tokens, released in '\n",
      "                'September of 2024.',\n",
      " 'display_name': 'Gemini 1.5 Pro 002',\n",
      " 'input_token_limit': 2000000,\n",
      " 'name': 'models/gemini-1.5-pro-002',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '002'}\n",
      "------------------------------\n",
      "{'description': 'Stable version of Gemini 1.5 Pro, our mid-size multimodal '\n",
      "                'model that supports up to 2 million tokens, released in May '\n",
      "                'of 2024.',\n",
      " 'display_name': 'Gemini 1.5 Pro',\n",
      " 'input_token_limit': 2000000,\n",
      " 'name': 'models/gemini-1.5-pro',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Alias that points to the most recent production '\n",
      "                '(non-experimental) release of Gemini 1.5 Flash, our fast and '\n",
      "                'versatile multimodal model for scaling across diverse tasks.',\n",
      " 'display_name': 'Gemini 1.5 Flash Latest',\n",
      " 'input_token_limit': 1000000,\n",
      " 'name': 'models/gemini-1.5-flash-latest',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Alias that points to the most recent stable version of Gemini '\n",
      "                '1.5 Flash, our fast and versatile multimodal model for '\n",
      "                'scaling across diverse tasks.',\n",
      " 'display_name': 'Gemini 1.5 Flash',\n",
      " 'input_token_limit': 1000000,\n",
      " 'name': 'models/gemini-1.5-flash',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Stable version of Gemini 1.5 Flash, our fast and versatile '\n",
      "                'multimodal model for scaling across diverse tasks, released '\n",
      "                'in September of 2024.',\n",
      " 'display_name': 'Gemini 1.5 Flash 002',\n",
      " 'input_token_limit': 1000000,\n",
      " 'name': 'models/gemini-1.5-flash-002',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '002'}\n",
      "------------------------------\n",
      "{'description': 'Stable version of Gemini 1.5 Flash-8B, our smallest and most '\n",
      "                'cost effective Flash model, released in October of 2024.',\n",
      " 'display_name': 'Gemini 1.5 Flash-8B',\n",
      " 'input_token_limit': 1000000,\n",
      " 'name': 'models/gemini-1.5-flash-8b',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['createCachedContent',\n",
      "                                  'generateContent',\n",
      "                                  'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Stable version of Gemini 1.5 Flash-8B, our smallest and most '\n",
      "                'cost effective Flash model, released in October of 2024.',\n",
      " 'display_name': 'Gemini 1.5 Flash-8B 001',\n",
      " 'input_token_limit': 1000000,\n",
      " 'name': 'models/gemini-1.5-flash-8b-001',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['createCachedContent',\n",
      "                                  'generateContent',\n",
      "                                  'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Alias that points to the most recent production '\n",
      "                '(non-experimental) release of Gemini 1.5 Flash-8B, our '\n",
      "                'smallest and most cost effective Flash model, released in '\n",
      "                'October of 2024.',\n",
      " 'display_name': 'Gemini 1.5 Flash-8B Latest',\n",
      " 'input_token_limit': 1000000,\n",
      " 'name': 'models/gemini-1.5-flash-8b-latest',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['createCachedContent',\n",
      "                                  'generateContent',\n",
      "                                  'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.5 Pro Preview 03-25',\n",
      " 'display_name': 'Gemini 2.5 Pro Preview 03-25',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.5-pro-preview-03-25',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5-preview-03-25'}\n",
      "------------------------------\n",
      "{'description': 'Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      " 'display_name': 'Gemini 2.5 Flash Preview 05-20',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.5-flash-preview-05-20',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5-preview-05-20'}\n",
      "------------------------------\n",
      "{'description': 'Stable version of Gemini 2.5 Flash, our mid-size multimodal '\n",
      "                'model that supports up to 1 million tokens, released in June '\n",
      "                'of 2025.',\n",
      " 'display_name': 'Gemini 2.5 Flash',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.5-flash',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite',\n",
      " 'display_name': 'Gemini 2.5 Flash-Lite Preview 06-17',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.5-flash-lite-preview-06-17',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5-preview-06-17'}\n",
      "------------------------------\n",
      "{'description': 'Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
      " 'display_name': 'Gemini 2.5 Pro Preview 05-06',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.5-pro-preview-05-06',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5-preview-05-06'}\n",
      "------------------------------\n",
      "{'description': 'Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
      " 'display_name': 'Gemini 2.5 Pro Preview',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.5-pro-preview-06-05',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5-preview-06-05'}\n",
      "------------------------------\n",
      "{'description': 'Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
      " 'display_name': 'Gemini 2.5 Pro',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.5-pro',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.0 Flash Experimental',\n",
      " 'display_name': 'Gemini 2.0 Flash Experimental',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash-exp',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'bidiGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.0'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.0 Flash',\n",
      " 'display_name': 'Gemini 2.0 Flash',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.0'}\n",
      "------------------------------\n",
      "{'description': 'Stable version of Gemini 2.0 Flash, our fast and versatile '\n",
      "                'multimodal model for scaling across diverse tasks, released '\n",
      "                'in January of 2025.',\n",
      " 'display_name': 'Gemini 2.0 Flash 001',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash-001',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.0'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.0 Flash (Image Generation) Experimental',\n",
      " 'display_name': 'Gemini 2.0 Flash (Image Generation) Experimental',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash-exp-image-generation',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'bidiGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.0'}\n",
      "------------------------------\n",
      "{'description': 'Stable version of Gemini 2.0 Flash-Lite',\n",
      " 'display_name': 'Gemini 2.0 Flash-Lite 001',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash-lite-001',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.0'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.0 Flash-Lite',\n",
      " 'display_name': 'Gemini 2.0 Flash-Lite',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash-lite',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.0'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.0 Flash Preview Image Generation',\n",
      " 'display_name': 'Gemini 2.0 Flash Preview Image Generation',\n",
      " 'input_token_limit': 32768,\n",
      " 'name': 'models/gemini-2.0-flash-preview-image-generation',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.0'}\n",
      "------------------------------\n",
      "{'description': 'Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      " 'display_name': 'Gemini 2.0 Flash-Lite Preview 02-05',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash-lite-preview-02-05',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': 'preview-02-05'}\n",
      "------------------------------\n",
      "{'description': 'Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      " 'display_name': 'Gemini 2.0 Flash-Lite Preview',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash-lite-preview',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 40,\n",
      " 'top_p': 0.95,\n",
      " 'version': 'preview-02-05'}\n",
      "------------------------------\n",
      "{'description': 'Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      " 'display_name': 'Gemini 2.0 Pro Experimental',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-pro-exp',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5-exp-03-25'}\n",
      "------------------------------\n",
      "{'description': 'Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      " 'display_name': 'Gemini 2.0 Pro Experimental 02-05',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-pro-exp-02-05',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5-exp-03-25'}\n",
      "------------------------------\n",
      "{'description': 'Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      " 'display_name': 'Gemini Experimental 1206',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-exp-1206',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5-exp-03-25'}\n",
      "------------------------------\n",
      "{'description': 'Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      " 'display_name': 'Gemini 2.5 Flash Preview 05-20',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash-thinking-exp-01-21',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5-preview-05-20'}\n",
      "------------------------------\n",
      "{'description': 'Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      " 'display_name': 'Gemini 2.5 Flash Preview 05-20',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash-thinking-exp',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5-preview-05-20'}\n",
      "------------------------------\n",
      "{'description': 'Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      " 'display_name': 'Gemini 2.5 Flash Preview 05-20',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash-thinking-exp-1219',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['generateContent',\n",
      "                                  'countTokens',\n",
      "                                  'createCachedContent',\n",
      "                                  'batchGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.5-preview-05-20'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.5 Flash Preview TTS',\n",
      " 'display_name': 'Gemini 2.5 Flash Preview TTS',\n",
      " 'input_token_limit': 8192,\n",
      " 'name': 'models/gemini-2.5-flash-preview-tts',\n",
      " 'output_token_limit': 16384,\n",
      " 'supported_generation_methods': ['countTokens', 'generateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': 'gemini-2.5-flash-exp-tts-2025-05-19'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.5 Pro Preview TTS',\n",
      " 'display_name': 'Gemini 2.5 Pro Preview TTS',\n",
      " 'input_token_limit': 8192,\n",
      " 'name': 'models/gemini-2.5-pro-preview-tts',\n",
      " 'output_token_limit': 16384,\n",
      " 'supported_generation_methods': ['countTokens', 'generateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': 'gemini-2.5-pro-preview-tts-2025-05-19'}\n",
      "------------------------------\n",
      "{'description': 'LearnLM 2.0 Flash Experimental',\n",
      " 'display_name': 'LearnLM 2.0 Flash Experimental',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/learnlm-2.0-flash-experimental',\n",
      " 'output_token_limit': 32768,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '2.0'}\n",
      "------------------------------\n",
      "{'description': '',\n",
      " 'display_name': 'Gemma 3 1B',\n",
      " 'input_token_limit': 32768,\n",
      " 'name': 'models/gemma-3-1b-it',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': '',\n",
      " 'display_name': 'Gemma 3 4B',\n",
      " 'input_token_limit': 32768,\n",
      " 'name': 'models/gemma-3-4b-it',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': '',\n",
      " 'display_name': 'Gemma 3 12B',\n",
      " 'input_token_limit': 32768,\n",
      " 'name': 'models/gemma-3-12b-it',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': '',\n",
      " 'display_name': 'Gemma 3 27B',\n",
      " 'input_token_limit': 131072,\n",
      " 'name': 'models/gemma-3-27b-it',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': '',\n",
      " 'display_name': 'Gemma 3n E4B',\n",
      " 'input_token_limit': 8192,\n",
      " 'name': 'models/gemma-3n-e4b-it',\n",
      " 'output_token_limit': 2048,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': '',\n",
      " 'display_name': 'Gemma 3n E2B',\n",
      " 'input_token_limit': 8192,\n",
      " 'name': 'models/gemma-3n-e2b-it',\n",
      " 'output_token_limit': 2048,\n",
      " 'supported_generation_methods': ['generateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Obtain a distributed representation of a text.',\n",
      " 'display_name': 'Embedding 001',\n",
      " 'input_token_limit': 2048,\n",
      " 'name': 'models/embedding-001',\n",
      " 'output_token_limit': 1,\n",
      " 'supported_generation_methods': ['embedContent'],\n",
      " 'temperature': None,\n",
      " 'top_k': None,\n",
      " 'top_p': None,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Obtain a distributed representation of a text.',\n",
      " 'display_name': 'Text Embedding 004',\n",
      " 'input_token_limit': 2048,\n",
      " 'name': 'models/text-embedding-004',\n",
      " 'output_token_limit': 1,\n",
      " 'supported_generation_methods': ['embedContent'],\n",
      " 'temperature': None,\n",
      " 'top_k': None,\n",
      " 'top_p': None,\n",
      " 'version': '004'}\n",
      "------------------------------\n",
      "{'description': 'Obtain a distributed representation of a text.',\n",
      " 'display_name': 'Gemini Embedding Experimental 03-07',\n",
      " 'input_token_limit': 8192,\n",
      " 'name': 'models/gemini-embedding-exp-03-07',\n",
      " 'output_token_limit': 1,\n",
      " 'supported_generation_methods': ['embedContent',\n",
      "                                  'countTextTokens',\n",
      "                                  'countTokens'],\n",
      " 'temperature': None,\n",
      " 'top_k': None,\n",
      " 'top_p': None,\n",
      " 'version': 'exp-03-07'}\n",
      "------------------------------\n",
      "{'description': 'Obtain a distributed representation of a text.',\n",
      " 'display_name': 'Gemini Embedding Experimental',\n",
      " 'input_token_limit': 8192,\n",
      " 'name': 'models/gemini-embedding-exp',\n",
      " 'output_token_limit': 1,\n",
      " 'supported_generation_methods': ['embedContent',\n",
      "                                  'countTextTokens',\n",
      "                                  'countTokens'],\n",
      " 'temperature': None,\n",
      " 'top_k': None,\n",
      " 'top_p': None,\n",
      " 'version': 'exp-03-07'}\n",
      "------------------------------\n",
      "{'description': 'Obtain a distributed representation of a text.',\n",
      " 'display_name': 'Gemini Embedding 001',\n",
      " 'input_token_limit': 2048,\n",
      " 'name': 'models/gemini-embedding-001',\n",
      " 'output_token_limit': 1,\n",
      " 'supported_generation_methods': ['embedContent',\n",
      "                                  'countTextTokens',\n",
      "                                  'countTokens'],\n",
      " 'temperature': None,\n",
      " 'top_k': None,\n",
      " 'top_p': None,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Model trained to return answers to questions that are '\n",
      "                'grounded in provided sources, along with estimating '\n",
      "                'answerable probability.',\n",
      " 'display_name': 'Model that performs Attributed Question Answering.',\n",
      " 'input_token_limit': 7168,\n",
      " 'name': 'models/aqa',\n",
      " 'output_token_limit': 1024,\n",
      " 'supported_generation_methods': ['generateAnswer'],\n",
      " 'temperature': 0.2,\n",
      " 'top_k': 40,\n",
      " 'top_p': 1.0,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Vertex served Imagen 3.0 002 model',\n",
      " 'display_name': 'Imagen 3.0 002 model',\n",
      " 'input_token_limit': 480,\n",
      " 'name': 'models/imagen-3.0-generate-002',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['predict'],\n",
      " 'temperature': None,\n",
      " 'top_k': None,\n",
      " 'top_p': None,\n",
      " 'version': '002'}\n",
      "------------------------------\n",
      "{'description': 'Vertex served Imagen 4.0 model',\n",
      " 'display_name': 'Imagen 4 (Preview)',\n",
      " 'input_token_limit': 480,\n",
      " 'name': 'models/imagen-4.0-generate-preview-06-06',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['predict'],\n",
      " 'temperature': None,\n",
      " 'top_k': None,\n",
      " 'top_p': None,\n",
      " 'version': '01'}\n",
      "------------------------------\n",
      "{'description': 'Vertex served Imagen 4.0 ultra model',\n",
      " 'display_name': 'Imagen 4 Ultra (Preview)',\n",
      " 'input_token_limit': 480,\n",
      " 'name': 'models/imagen-4.0-ultra-generate-preview-06-06',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['predict'],\n",
      " 'temperature': None,\n",
      " 'top_k': None,\n",
      " 'top_p': None,\n",
      " 'version': '01'}\n",
      "------------------------------\n",
      "{'description': 'Vertex served Veo 2 model. Access to this model requires '\n",
      "                'billing to be enabled on the associated Google Cloud Platform '\n",
      "                'account. Please visit '\n",
      "                'https://console.cloud.google.com/billing to enable it.',\n",
      " 'display_name': 'Veo 2',\n",
      " 'input_token_limit': 480,\n",
      " 'name': 'models/veo-2.0-generate-001',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['predictLongRunning'],\n",
      " 'temperature': None,\n",
      " 'top_k': None,\n",
      " 'top_p': None,\n",
      " 'version': '2.0'}\n",
      "------------------------------\n",
      "{'description': 'Veo 3 preview. Access to this model requires billing to be '\n",
      "                'enabled on the associated Google Cloud Platform account. '\n",
      "                'Please visit https://console.cloud.google.com/billing to '\n",
      "                'enable it.',\n",
      " 'display_name': 'Veo 3',\n",
      " 'input_token_limit': 480,\n",
      " 'name': 'models/veo-3.0-generate-preview',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['predictLongRunning'],\n",
      " 'temperature': None,\n",
      " 'top_k': None,\n",
      " 'top_p': None,\n",
      " 'version': '3.0'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      " 'display_name': 'Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      " 'input_token_limit': 131072,\n",
      " 'name': 'models/gemini-2.5-flash-preview-native-audio-dialog',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['countTokens', 'bidiGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': 'gemini-2.5-flash-preview-native-audio-dialog-2025-05-19'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      " 'display_name': 'Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      " 'input_token_limit': 131072,\n",
      " 'name': 'models/gemini-2.5-flash-exp-native-audio-thinking-dialog',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['countTokens', 'bidiGenerateContent'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': 'gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.0 Flash 001',\n",
      " 'display_name': 'Gemini 2.0 Flash 001',\n",
      " 'input_token_limit': 131072,\n",
      " 'name': 'models/gemini-2.0-flash-live-001',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_generation_methods': ['bidiGenerateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Gemini Live 2.5 Flash Preview',\n",
      " 'display_name': 'Gemini Live 2.5 Flash Preview',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-live-2.5-flash-preview',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['bidiGenerateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n",
      "{'description': 'Gemini 2.5 Flash Live Preview',\n",
      " 'display_name': 'Gemini 2.5 Flash Live Preview',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.5-flash-live-preview',\n",
      " 'output_token_limit': 65536,\n",
      " 'supported_generation_methods': ['bidiGenerateContent', 'countTokens'],\n",
      " 'temperature': 1.0,\n",
      " 'top_k': 64,\n",
      " 'top_p': 0.95,\n",
      " 'version': '001'}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from pprint import pprint # For pretty printing the model details\n",
    "\n",
    "# Ensure your API key is set as an environment variable\n",
    "# or configure it directly (though environment variable is preferred for security)\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_BRAND_NEW_API_KEY_HERE\" # Assuming you've set this\n",
    "\n",
    "# Configure the genai library with your API key\n",
    "# If using os.environ, you don't need to pass api_key explicitly here\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "print(\"Listing available Gemini models and their supported methods:\")\n",
    "try:\n",
    "    for model in genai.list_models():\n",
    "        # You can filter by supported generation methods if needed,\n",
    "        # for example, to only see models that can generate content.\n",
    "        # if 'generateContent' in model.supported_generation_methods:\n",
    "        pprint({\n",
    "            \"name\": model.name,\n",
    "            \"description\": model.description,\n",
    "            \"supported_generation_methods\": model.supported_generation_methods,\n",
    "            \"input_token_limit\": model.input_token_limit,\n",
    "            \"output_token_limit\": model.output_token_limit,\n",
    "            \"temperature\": getattr(model, 'temperature', 'N/A'), # Not all models have this\n",
    "            \"top_p\": getattr(model, 'top_p', 'N/A'),\n",
    "            \"top_k\": getattr(model, 'top_k', 'N/A'),\n",
    "            \"version\": model.version,\n",
    "            \"display_name\": model.display_name\n",
    "        })\n",
    "        print(\"-\" * 30) # Separator for readability\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while listing models: {e}\")\n",
    "    print(\"Please ensure your API key is valid and the 'Generative Language API' is enabled in your Google Cloud Project.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCTJvIzYVqU_",
    "outputId": "0abd2aea-b432-4882-ad05-acaa870a8dff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Imagine a light switch. It can be either ON or OFF, right?  That\\'s how regular computers work: they use bits, which are either 0 or 1.\\n\\nQuantum computers are different. They use **qubits**.  A qubit can be 0, 1, or *both at the same time* ‚Äì a concept called **superposition**.  Think of it like a dimmer switch: it can be fully on, fully off, or anywhere in between.\\n\\nThis \"both at the same time\" ability lets quantum computers explore many possibilities simultaneously.  It\\'s like checking every path in a maze at once, instead of trying them one by one.\\n\\nAnother key concept is **entanglement**. Imagine two qubits linked magically: if you measure one and it\\'s 0, you instantly know the other is 1, no matter how far apart they are. This interconnectedness allows for incredibly powerful calculations.\\n\\nBecause of superposition and entanglement, quantum computers have the potential to solve certain problems much faster than even the most powerful regular computers.  These problems include:\\n\\n* **Drug discovery and materials science:** Simulating molecules to design new drugs and materials.\\n* **Financial modeling:**  Analyzing complex financial markets.\\n* **Cryptography:** Breaking current encryption methods and creating new, unbreakable ones.\\n\\nHowever, quantum computers are still in their early stages. They\\'re very difficult to build and maintain, and they\\'re not good at everything.  They\\'re specialized tools for specific types of problems.  They won\\'t replace your laptop anytime soon!' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--732425b3-d57f-4519-a715-435a083022b7-0' usage_metadata={'input_tokens': 7, 'output_tokens': 326, 'total_tokens': 333, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Run a basic prompt\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7)\n",
    "\n",
    "response = llm.invoke(\"Explain quantum computing in simple terms.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POyXu_v9Z1FJ"
   },
   "source": [
    "## Simple Invocation (Chat):\n",
    "You can now send a simple message to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kbx2Sjp7Z10T",
    "outputId": "de47a419-ac4a-42f3-f635-9000adad84d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine a parrot that has read every book and website ever written.  It doesn't *understand* the meaning in the same way a human does, but it's learned to predict which words come next in a sentence based on all that it's \"read.\"\n",
      "\n",
      "That's basically a large language model (LLM).  It's a computer program that's been trained on massive amounts of text data, allowing it to generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.  It's good at mimicking human language, but it doesn't actually *think* or *understand* like a human.  It's just really good at pattern recognition.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "message = HumanMessage(content=\"Explain large language models in a simple way.\")\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GyYpqL1bxQ6"
   },
   "source": [
    "## LangChain Chains:\n",
    "In LangChain, Chains are sequences of modular components (like Language Models, Prompt Templates, Output Parsers, or even other Chains) linked together to automate multi-step tasks. They allow you to create a streamlined workflow where the output of one component seamlessly becomes the input for the next.\n",
    "\n",
    "\n",
    "\n",
    ">LangChain uses the pipe operator (|) to elegantly compose these components into a chain. This is part of the LangChain Expression Language (LCEL), which is designed for building flexible and production-ready applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ1ClWrdZ7I-"
   },
   "source": [
    "## Prompt Templates:\n",
    "For more structured interactions, use ChatPromptTemplate. This allows you to define prompts with placeholders that you can dynamically fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2tCQ3MxcHmh",
    "outputId": "bc1e92bc-fadb-4ca9-dc46-54ee6a507927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Invoking the Chain ---\n",
      "Fun Fact: There are more stars in the universe than grains of sand on all the beaches on Earth.  \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# 1. Initialize the Language Model (ChatModel for conversational LLMs)\n",
    "# Using a model from your list that supports 'generateContent'\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7)\n",
    "\n",
    "\n",
    "# 2. Define a Prompt Template\n",
    "# This template takes a 'topic' as input and formats it for the LLM.\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a fun fact about {topic}.\")\n",
    "\n",
    "# 3. Define an Output Parser (to get a simple string from the LLM's response)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4. Create the Chain using the pipe operator\n",
    "# Input -> Prompt -> LLM -> Parser -> Output\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "print(\"\\n--- Invoking the Chain ---\")\n",
    "\n",
    "# Invoke the chain with an input dictionary\n",
    "result = chain.invoke({\"topic\": \"the universe\"})\n",
    "print(f\"Fun Fact: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIiUWuEme3VP"
   },
   "source": [
    "# üß† LangChain Memory\n",
    "\n",
    "## üìå Purpose\n",
    "- Enables LLM applications to retain context across multiple turns or interactions.\n",
    "- Makes conversations more coherent and personalized.\n",
    "- LLMs are stateless by default ‚Äî Memory adds \"state.\"\n",
    "\n",
    "## ‚öôÔ∏è How It Works\n",
    "- Stores and retrieves past messages or extracted information.\n",
    "- Injects relevant context into new prompts.\n",
    "\n",
    "## üß© Key Types of Memory\n",
    "\n",
    "### 1. **ConversationBufferMemory**\n",
    "- Stores **entire raw conversation history**.\n",
    "- Simple but can become large and costly.\n",
    "- **Use case**: Short, simple chat sessions.\n",
    "\n",
    "### 2. **ConversationBufferWindowMemory**\n",
    "- Stores only the **k most recent interactions** (sliding window).\n",
    "- Prevents memory bloat in long conversations.\n",
    "- **Use case**: Chatbots needing only recent context (e.g., food delivery assistant).\n",
    "\n",
    "### 3. **ConversationSummaryMemory**\n",
    "- Summarizes conversation progressively using an LLM.\n",
    "- Keeps token count low for long sessions.\n",
    "- **Use case**: Executive assistants, customer support bots.\n",
    "\n",
    "### 4. **ConversationSummaryBufferMemory**\n",
    "- Combines **window memory + summary memory**.\n",
    "- Keeps k recent messages and a summary of older ones.\n",
    "- **Use case**: Balances short-term context with long-term memory.\n",
    "\n",
    "### 5. **ConversationTokenBufferMemory**\n",
    "- Limits stored history based on **maximum token count**.\n",
    "- Token-efficient alternative to message-based windows.\n",
    "- **Use case**: Cost-sensitive applications.\n",
    "\n",
    "### 6. **EntityMemory**\n",
    "- Extracts and remembers facts about specific entities.\n",
    "- Builds a contextual knowledge base.\n",
    "- **Use case**: Personalized bots that recall user preferences.\n",
    "\n",
    "## üõ†Ô∏è Implementation\n",
    "- Create a memory object and pass it to a chain or agent.\n",
    "- Memory reads context before an LLM call and writes new interactions afterward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l77ZlYdrh-RQ"
   },
   "source": [
    "üìå Explanation:\n",
    "\n",
    "- ChatPromptTemplate.from_messages(...): This creates a template for dialogue-style prompts.\n",
    "\n",
    "- MessagesPlaceholder(...): This is a dynamic section. It tells the model where to insert remembered conversation history from the memory object.\n",
    "\n",
    "- (\"human\", \"{input}\"): This tag specifies that you're adding a new human message to the chat. {input} will be replaced by whatever text the user sends in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8H5ftMdDe31s",
    "outputId": "b059e891-649e-4cb5-f27a-b38ad75d7813"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-45-3131928554.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True) # return_messages=True is good for chat models\n",
      "/tmp/ipython-input-45-3131928554.py:18: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  conversation = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7)\n",
    "\n",
    "# Define memory\n",
    "memory = ConversationBufferMemory(return_messages=True) # return_messages=True is good for chat models\n",
    "\n",
    "# Define prompt with a placeholder for history\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"history\"), # This is where memory will inject past messages\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create an LLMChain with memory\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False # Set to True to see the full prompt with history\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-HdfOeXhoqE"
   },
   "source": [
    ">invoke() is the method that runs the chain with your input.\n",
    "\n",
    ">It returns a dictionary with keys like \"text\", \"log\", or metadata depending on your setup.\n",
    "\n",
    "> [\"text\"] pulls out just the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GP2PZIQXhnyI",
    "outputId": "e828e381-1a50-4277-f949-a418888884bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation 1:\n",
      "Hi Alex, it's nice to meet you!  How can I help you today?\n",
      "Conversation 2:\n",
      "Your name is Alex.\n"
     ]
    }
   ],
   "source": [
    "# Interact\n",
    "print(\"Conversation 1:\")\n",
    "print(conversation.invoke({\"input\": \"Hi, my name is Alex.\"})[\"text\"])\n",
    "print(\"Conversation 2:\")\n",
    "print(conversation.invoke({\"input\": \"What is my name?\"})[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3pOCXmZg7w2",
    "outputId": "fe6e2645-5d62-4599-dca0-d287bb30ac0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory Buffer: [HumanMessage(content='Hi, my name is Alex.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Alex, it's nice to meet you!  How can I help you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Alex.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# You can also directly inspect the memory buffer\n",
    "print(\"\\nMemory Buffer:\", memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiFQmFwei2PG"
   },
   "source": [
    "# üîß LangChain Agents\n",
    "\n",
    "### üß† Purpose\n",
    "Enable Language Models to **reason and take actions** by dynamically deciding which tools to use and in what order to achieve a given goal.  \n",
    "Unlike Chains, which follow a **fixed sequence**, Agents have a **dynamic \"thought\" process**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Core Components\n",
    "\n",
    "- **LLM (Language Model)**  \n",
    "  The \"brain\" that performs reasoning and decides actions.\n",
    "\n",
    "- **Tools**  \n",
    "  Functions or external APIs that the agent can call to interact with the world  \n",
    "  (e.g., Google Search, Calculator, a custom database query tool).\n",
    "\n",
    "- **Agent Executor**  \n",
    "  The runtime that manages the agent's loop:  \n",
    "  LLM decides an action ‚Üí executes the tool ‚Üí observes the result ‚Üí repeats until the task is done or a stopping condition is met.\n",
    "\n",
    "- **Memory (Optional but important)**  \n",
    "  Maintains context across turns, similar to how memory works for Chains.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ How It Works: ReAct Pattern\n",
    "\n",
    "Agents typically use the **Reasoning and Acting (ReAct)** pattern:\n",
    "\n",
    "1. **Reason** about the user's request\n",
    "2. **Decide** which tool to use and what input to provide\n",
    "3. **Act** by executing the tool\n",
    "4. **Observe** the result from the tool\n",
    "5. **Repeat** reasoning using the observation or conclude with a final answer\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Key Agent Types / Architectures\n",
    "\n",
    "| Agent Type | Description |\n",
    "|------------|-------------|\n",
    "| **Zero-shot ReAct** | Simple, immediate decision-making. Great for single-turn tool use |\n",
    "| **OpenAI Functions/Tools Agent** | Leverages models fine-tuned for structured tool calls |\n",
    "| **Structured Chat Agent** | Designed for multi-input tools with full support for chat history |\n",
    "| **LangGraph** | LangChain's framework for building complex, stateful, controllable multi-agent workflows (evolution of AgentExecutor) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "BN3WEAkziZuR"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import Tool\n",
    "#initialize_agent: Creates an agent that can use tools.\n",
    "# Tool: Lets us define tools like calculator or search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8H60a1sml5SM"
   },
   "source": [
    "> \"memory\" is a special keyword that tells SQLite to create the database in your machine‚Äôs RAM.\n",
    "\n",
    ">It‚Äôs super fast ‚ö° because there's no disk I/O.\n",
    "\n",
    ">It‚Äôs temporary ‚Äî once your script finishes running or the connection is closed, the database disappears.\n",
    "\n",
    ">Great for testing, unit tests, or short-lived data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "lVc7lDUUkYqS"
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create a sample in-memory SQLite database\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "cursor = conn.cursor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "iP-6FWnNmKer"
   },
   "outputs": [],
   "source": [
    "# Create a simple employees table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE employees (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    department TEXT,\n",
    "    salary INTEGER\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample data\n",
    "cursor.executemany(\"\"\"\n",
    "INSERT INTO employees (name, department, salary) VALUES (?, ?, ?)\n",
    "\"\"\", [\n",
    "    (\"Alice\", \"Engineering\", 100000),\n",
    "    (\"Bob\", \"Engineering\", 95000),\n",
    "    (\"Charlie\", \"HR\", 60000),\n",
    "    (\"Diana\", \"HR\", 62000),\n",
    "    (\"Eve\", \"Marketing\", 75000),\n",
    "])\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIE4biRhmVQZ"
   },
   "source": [
    " üîå LangChain SQL Tool Setup\n",
    "python\n",
    "Copy code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-W8OI-JDmO6r",
    "outputId": "ee4cf18f-36d5-4e3c-b679-2ca258adee86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-53-1319336336.py:9: LangChainDeprecationWarning: The class `QuerySQLDataBaseTool` was deprecated in LangChain 0.3.12 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-community package and should be used instead. To use it run `pip install -U :class:`~langchain-community` and import as `from :class:`~langchain_community.tools import QuerySQLDatabaseTool``.\n",
      "  sql_tool = QuerySQLDataBaseTool(db=db)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "\n",
    "# Connect LangChain to the SQLite database\n",
    "#\"sqlite:///:memory:\" is the standard URI for in-memory SQLite databases.\n",
    "db = SQLDatabase.from_uri(\"sqlite:///:memory:\")\n",
    "\n",
    "# Create a SQL query tool\n",
    "sql_tool = QuerySQLDataBaseTool(db=db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYnzFg9UnVXv"
   },
   "source": [
    "| Step             | Meaning                                        |\n",
    "| ---------------- | ---------------------------------------------- |\n",
    "| **Thought**      | Gemini \"thinks\" about how to solve the request |\n",
    "| **Action**       | Gemini decides to call the SQL tool            |\n",
    "| **Action Input** | The exact SQL it tells the tool to run         |\n",
    "| **Observation**  | The output from the tool (the query result)    |\n",
    "| **Final Answer** | The formatted final reply returned to the user |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IHUTTeLBmXtj",
    "outputId": "eefcbb85-ea3f-4c5f-dda9-3f0cb6c98a91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-54-1451789015.py:12: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n",
      "/tmp/ipython-input-54-1451789015.py:23: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = agent.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to query the database to find the average salary of employees in the Engineering department.  I'll assume there's a table named 'employees' with columns 'department' and 'salary'.\n",
      "\n",
      "Action: sql_db_query\n",
      "Action Input: ```sql\n",
      "SELECT AVG(salary)\n",
      "FROM employees\n",
      "WHERE department = 'Engineering';\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mError: (sqlite3.OperationalError) near \"```sql\n",
      "SELECT AVG(salary)\n",
      "FROM employees\n",
      "WHERE department = 'Engineering';\n",
      "```\": syntax error\n",
      "[SQL: ```sql\n",
      "SELECT AVG(salary)\n",
      "FROM employees\n",
      "WHERE department = 'Engineering';\n",
      "```]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The error message indicates a syntax error.  The backticks are likely the problem. I'll remove them and try again.  I'll also add error handling in case the 'employees' table or 'salary' column doesn't exist.\n",
      "\n",
      "Action: sql_db_query\n",
      "Action Input: ```sql\n",
      "SELECT AVG(salary)\n",
      "FROM employees\n",
      "WHERE department = 'Engineering';\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mError: (sqlite3.OperationalError) near \"```sql\n",
      "SELECT AVG(salary)\n",
      "FROM employees\n",
      "WHERE department = 'Engineering';\n",
      "```\": syntax error\n",
      "[SQL: ```sql\n",
      "SELECT AVG(salary)\n",
      "FROM employees\n",
      "WHERE department = 'Engineering';\n",
      "```]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The error persists.  Let's try a different approach. I'll remove the extra ```sql` and see if that solves the syntax error.  I'll also check if the table and column actually exist.\n",
      "\n",
      "Action: sql_db_query\n",
      "Action Input: ```sql\n",
      "SELECT name FROM sqlite_master WHERE type='table' AND name='employees';\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mError: (sqlite3.OperationalError) near \"```sql\n",
      "SELECT name FROM sqlite_master WHERE type='table' AND name='employees';\n",
      "```\": syntax error\n",
      "[SQL: ```sql\n",
      "SELECT name FROM sqlite_master WHERE type='table' AND name='employees';\n",
      "```]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The error persists. It seems there's a problem with how the SQL query is being passed to the `sql_db_query` function.  The backticks are likely interfering. I need to remove them and ensure the query is being passed correctly.  I'll also add error handling to check for the table and column existence before attempting the average calculation.\n",
      "\n",
      "\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT name FROM sqlite_master WHERE type='table' AND name='employees';\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The previous attempts failed due to issues with how the SQL query was being passed to the function.  Let's try again without the backticks and extra formatting.  I'll also add a check to see if the 'employees' table exists before querying it.\n",
      "\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT name FROM sqlite_master WHERE type='table' AND name='employees';\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The previous attempts failed due to issues with how the SQL query was being passed to the function.  Let's try again without the backticks and extra formatting.  I'll also add a check to see if the 'employees' table exists before querying it.  If the table doesn't exist, I'll return an appropriate message.\n",
      "\n",
      "\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT name FROM sqlite_master WHERE type='table' AND name='employees';\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The previous attempts failed due to issues with how the SQL query was being passed to the function.  Let's try again without the backticks and extra formatting.  I'll also add a check to see if the 'employees' table exists before querying it.  If the table doesn't exist, I'll return an appropriate message.  If the table exists, I'll then check if the 'salary' and 'department' columns exist before calculating the average salary.\n",
      "\n",
      "\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT name FROM sqlite_master WHERE type='table' AND name='employees';\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The previous attempts failed because I didn't have access to a real database.  The error messages were simulated. To answer the question, I need a real database and table to query.  Without that, I cannot provide a numerical answer.\n",
      "\n",
      "Final Answer: I cannot answer the question without access to a database containing the necessary table and data.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I cannot answer the question without access to a database containing the necessary table and data.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary LangChain components\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# Step 1: Register your tools\n",
    "# Tools are functions or APIs the agent can call to interact with external data or services.\n",
    "# In this case, you‚Äôre registering a single tool, likely connected to a SQL database.\n",
    "tools = [sql_tool]\n",
    "\n",
    "# Step 2: Create the agent\n",
    "# This agent will be responsible for handling tasks dynamically and reasoning about which tool to use.\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,                            # List of tools the agent can use\n",
    "    llm=llm,                                # The language model (LLM) that drives decision-making\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Type of agent: uses ReAct pattern without needing examples\n",
    "    verbose=True                            # If set to True, prints internal reasoning steps for debugging\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query = \"What is the average salary of employees in the Engineering department?\"\n",
    "response = agent.run(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlBo1yownZya"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

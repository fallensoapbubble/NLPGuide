{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-gLxSr6kTSl"
   },
   "source": [
    "\n",
    "\n",
    "## üß† What is LangGraph?\n",
    "\n",
    "**LangGraph** is a Python library for building **stateful, multi-agent applications** powered by **graphs**. It's built on top of **LangChain**, and is great for modeling complex AI workflows where agents or tools need to talk to each other in a controlled sequence or loop.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Concepts\n",
    "\n",
    "1. **Graph-based Structure**\n",
    "\n",
    "   * You define nodes (functions or chains) and how they connect.\n",
    "   * Think of it as a flowchart of how data moves.\n",
    "\n",
    "2. **Stateful Execution**\n",
    "\n",
    "   * Each step can update or access a shared `state` (like memory).\n",
    "   * Useful for conversations or workflows needing memory.\n",
    "\n",
    "3. **Multi-Agent Workflows**\n",
    "\n",
    "   * Supports building systems where multiple LLM agents collaborate.\n",
    "   * Example: One agent writes, another reviews, another executes code.\n",
    "\n",
    "4. **Deterministic Paths**\n",
    "\n",
    "   * You control what node runs next based on output.\n",
    "   * Like an `if/else` condition in a flow.\n",
    "\n",
    "5. **Built-in Integration with LangChain Tools**\n",
    "\n",
    "   * You can use tools like retrieval, chains, and agents inside LangGraph nodes.\n",
    "\n",
    "6. **Supports Async Execution**\n",
    "\n",
    "   * Efficient for running tasks like API calls or LLMs in parallel.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## üß† Real-World Use Case: Document Review Bot\n",
    "\n",
    "**Example Steps:**\n",
    "\n",
    "1. Node A: Extract key points from document\n",
    "2. Node B: Summarize\n",
    "3. Node C: Send to reviewer\n",
    "4. Loop until approved\n",
    "\n",
    "**LangGraph lets you:**\n",
    "\n",
    "* Handle conditional loops (e.g., review not approved ‚Üí go back)\n",
    "* Maintain state (e.g., track number of revisions)\n",
    "* Use multiple agents for specialized tasks\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ Summary\n",
    "\n",
    "| Feature             | Description                                         |\n",
    "| ------------------- | --------------------------------------------------- |\n",
    "| Nodes               | Functions or LangChain Chains                       |\n",
    "| Edges               | Define transitions between nodes                    |\n",
    "| State               | Shared dictionary that nodes update                 |\n",
    "| Looping & Branching | Supports feedback loops and conditionals            |\n",
    "| Multi-Agent Ready   | Easily plug in different agents for different tasks |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvgnZxXSlziD"
   },
   "source": [
    "\n",
    "\n",
    "## üîÑ LangChain vs LangGraph\n",
    "\n",
    "| Feature                  | **LangChain**                                          | **LangGraph**                                                 |\n",
    "| ------------------------ | ------------------------------------------------------ | ------------------------------------------------------------- |\n",
    "| **Purpose**              | Build **LLM applications** using chains, tools, memory | Build **stateful, multi-step workflows** using a graph model  |\n",
    "| **Structure**            | Linear or branching **chains**                         | **Graphs** with nodes & edges, like a flowchart               |\n",
    "| **Control Flow**         | Mostly sequential or with some branching               | Complex logic with loops, conditions, and dynamic routing     |\n",
    "| **State Handling**       | Simple, usually passed step-to-step                    | Explicit **state dictionary** updated by each node            |\n",
    "| **Multi-Agent Support**  | Limited / manual                                       | Built-in, clean way to model multiple agents working together |\n",
    "| **Looping & Conditions** | Harder to manage                                       | Easy and native support                                       |\n",
    "| **Ideal Use Case**       | Simple chains, tool use, memory, retrievers            | Complex workflows, agents, feedback loops                     |\n",
    "| **Async Support**        | Partial                                                | Strong (designed for concurrency)                             |\n",
    "| **Dependency**           | Standalone                                             | Built **on top of LangChain**                                 |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Think of it like this:\n",
    "\n",
    "* **LangChain** = good for building **simple pipelines**:\n",
    "  ‚Üí Input ‚Üí LLM ‚Üí Output\n",
    "  ‚Üí With some tools or memory in between.\n",
    "\n",
    "* **LangGraph** = good for **complex workflows**:\n",
    "  ‚Üí Node A ‚Üí Node B ‚Üí (if condition) Node C ‚Üí (loop) Node A\n",
    "  ‚Üí Can branch, loop, and handle multiple agents.\n",
    "\n",
    "\n",
    "\n",
    "## üèÅ TL;DR Summary\n",
    "\n",
    "* Use **LangChain** for simple, modular AI components.\n",
    "* Use **LangGraph** when you need **complex control flow**, **memory**, or **multiple agents** working together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naroFCMHmh_B"
   },
   "source": [
    "https://js.langchain.com/docs/introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDn1jygqmHeT",
    "outputId": "3ef6aaef-8928-4c18-96b1-1e68b475c977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Downloading langgraph-0.5.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.70)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.74-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (0.4.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
      "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.11.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain_core) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain_core) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
      "Downloading langgraph-0.5.4-py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\n",
      "Downloading langgraph_sdk-0.1.74-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
      "Successfully installed langgraph-0.5.4 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.74 ormsgpack-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94Db_frEF7LR"
   },
   "source": [
    "\n",
    "\n",
    "### üß† `import google.generativeai as genai`\n",
    "- This imports the `google.generativeai` library and gives it a shortcut name `genai`.\n",
    "- Think of it like adding a toolset that lets you use Google's powerful generative AI features‚Äîlike building chatbots, content creators, and more.\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ `from langchain_core.messages import HumanMessage, AIMessage`\n",
    "- Brings in two types of message formats:\n",
    "  - `HumanMessage`: represents what a user (human) says.\n",
    "  - `AIMessage`: represents what the AI replies.\n",
    "- Useful for creating realistic, chat-like interactions between people and AI systems.\n",
    "\n",
    "---\n",
    "\n",
    "### üîó `from langgraph.graph import StateGraph, END`\n",
    "- Part of the **LangGraph** library, which lets you build complex workflows and decision trees for your AI agents.\n",
    "- `StateGraph`: Helps define the flow or logic of your AI-powered app (like a flowchart in code).\n",
    "- `END`: A marker used to define the final point or completion of your graph.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è `from langchain_core.runnables import RunnableLambda`\n",
    "- Imports `RunnableLambda`, which lets you create simple, callable pieces of logic (functions).\n",
    "- You wrap your Python functions in `RunnableLambda` so they can be used smoothly inside LangChain workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "02RrS1C2FyWZ"
   },
   "outputs": [],
   "source": [
    "gkey=\"AIzaSyCOpoQvsNT6ylEd87-lY7-_b2YeiMfyaws\"\n",
    "\n",
    "\n",
    "genai.configure(api_key=gkey)\n",
    "\n",
    "# Initialize the Gemini model you want to use\n",
    "gemini_model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N_QQozqBF_Ts"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "slTfvX7MGBBo"
   },
   "outputs": [],
   "source": [
    "# Define shared state type\n",
    "class SupportState(dict):\n",
    "    issue: str\n",
    "    resolved: bool = False\n",
    "    department: str = None\n",
    "    history: list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "toEN7YtYGEx0"
   },
   "outputs": [],
   "source": [
    "# Receptionist node\n",
    "def receptionist_node(state: SupportState) -> SupportState:\n",
    "    user_input = state.get(\"issue\", \"\")\n",
    "    response = gemini_model.generate_content(\n",
    "        f\"The customer says: '{user_input}'. Ask them: 'Has your issue been resolved? (yes/no)'\"\n",
    "    )\n",
    "    print(\"Receptionist: Has your issue been resolved (yes or no)?\")\n",
    "    answer = input(\"User: \").lower()\n",
    "    state[\"resolved\"] = \"yes\" in answer\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kf0c6t8uGLL3"
   },
   "outputs": [],
   "source": [
    "# Supervisor node\n",
    "def supervisor_node(state: SupportState) -> SupportState:\n",
    "    issue = state.get(\"issue\", \"\")\n",
    "    response = gemini_model.generate_content(\n",
    "        f\"The issue is: '{issue}'. Route this to one of the following: technical, financial, or miscellaneous.\"\n",
    "    )\n",
    "    print(\"Supervisor decision:\", response.text)\n",
    "    # Simulate LLM decision parsing\n",
    "    if \"tech\" in response.text.lower():\n",
    "        state[\"department\"] = \"technical\"\n",
    "    elif \"financ\" in response.text.lower():\n",
    "        state[\"department\"] = \"financial\"\n",
    "    else:\n",
    "        state[\"department\"] = \"miscellaneous\"\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dwaPScWsGP9z"
   },
   "outputs": [],
   "source": [
    "# Advisor nodes\n",
    "def technical_advisor(state: SupportState):\n",
    "    print(\"üîß Technical Advisor: Helping with technical issue...\")\n",
    "    return state\n",
    "\n",
    "def financial_advisor(state: SupportState):\n",
    "    print(\"üí∞ Financial Advisor: Helping with billing or payment...\")\n",
    "    return state\n",
    "\n",
    "def misc_advisor(state: SupportState):\n",
    "    print(\"üß≠ Miscellaneous Advisor: Handling general questions...\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzy09sWJGXGo"
   },
   "source": [
    "‚úÖ Scenario:\n",
    "A receptionist agent asks the user if their issue is resolved.\n",
    "\n",
    "If yes, the conversation ends.\n",
    "\n",
    "If no, it escalates to a supervisor, who decides whether to redirect the user to:\n",
    "\n",
    "a technical,\n",
    "\n",
    "financial, or\n",
    "\n",
    "miscellaneous advisor.\n",
    "\n",
    "We'll simulate this using LangGraph with Google GenerativeAI (Gemini) via google.generativeai client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fC_hidriGSyi",
    "outputId": "eb51280a-7f72-4ed3-b8fb-2a94cc568959"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7ae65a71f290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(SupportState)\n",
    "\n",
    "builder.add_node(\"receptionist\", receptionist_node)\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"technical\", technical_advisor)\n",
    "builder.add_node(\"financial\", financial_advisor)\n",
    "builder.add_node(\"miscellaneous\", misc_advisor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw19sUwkGrMH"
   },
   "source": [
    "\n",
    "\n",
    "### üì¶ `from langgraph.graph import StateGraph, END`\n",
    "- **StateGraph**: This is a tool that lets you define a flow of states, like a roadmap of decisions or steps.\n",
    "  - Think of it like telling your AI, ‚ÄúFirst act like a receptionist. If needed, escalate to a supervisor. Then go to technical support,‚Äù etc.\n",
    "- **END**: This marks the end of the state flow‚Äîkind of like saying ‚ÄúThis is the final destination of this conversation.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è `builder = StateGraph(SupportState)`\n",
    "- You‚Äôre creating a **graph builder** that‚Äôs based on a state model called `SupportState`.\n",
    "- `SupportState` usually defines the structure of what data is being tracked‚Äîlike the current query, whether the issue is resolved, etc.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "- `\"receptionist\"`: First point of contact. Usually gathers basic info or redirects queries.\n",
    "- `\"supervisor\"`: Handles cases that are more complex or need escalation.\n",
    "- `\"technical\"`: Deals with tech-related issues (e.g., app not working).\n",
    "- `\"financial\"`: For billing or money-related questions.\n",
    "- `\"miscellaneous\"`: A catch-all for anything that doesn‚Äôt fit into other buckets.\n",
    "\n",
    "Each role is matched with a function or agent (`*_node`, `*_advisor`) that knows how to respond for that specific job.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ So what are you doing here?\n",
    "You‚Äôre building a smart assistant that can:\n",
    "- Route queries to different departments\n",
    "- Handle tasks in a structured way\n",
    "- Know when it has reached the final step\n",
    "\n",
    "It‚Äôs like building an AI-powered customer support call center‚Äîbut in code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sKH1j_d7Gw6d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define edges\n",
    "builder.set_entry_point(\"receptionist\")\n",
    "builder.add_conditional_edges(\n",
    "    \"receptionist\",\n",
    "    lambda state: END if state[\"resolved\"] else \"supervisor\"\n",
    ")\n",
    "builder.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda state: state[\"department\"]\n",
    ")\n",
    "\n",
    "builder.add_edge(\"technical\", END)\n",
    "builder.add_edge(\"financial\", END)\n",
    "builder.add_edge(\"miscellaneous\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLegMkLlHDAz"
   },
   "source": [
    "\n",
    "\n",
    "### üö™ `builder.set_entry_point(\"receptionist\")`\n",
    "- This tells your graph to **start** at the node named `\"receptionist\"`.\n",
    "- It acts like the front desk ‚Äî the entry point for handling incoming support queries.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§î `builder.add_conditional_edges(...)`\n",
    "This defines how the conversation should move forward depending on certain conditions (like decision-making).\n",
    "\n",
    "#### üß≠ From `\"receptionist\"`:\n",
    "```python\n",
    "lambda state: END if state[\"resolved\"] else \"supervisor\"\n",
    "```\n",
    "- This checks the `state` ‚Äî imagine it‚Äôs like a record of what‚Äôs happening so far.\n",
    "- If the issue is **resolved**, go straight to `END`.\n",
    "- If not, pass the conversation to the `\"supervisor\"` node for further help.\n",
    "\n",
    "#### üìÇ From `\"supervisor\"`:\n",
    "```python\n",
    "lambda state: state[\"department\"]\n",
    "```\n",
    "- Here, the graph looks at which **department** the user needs.\n",
    "- That could be `\"technical\"`, `\"financial\"`, or `\"miscellaneous\"` ‚Äî and routes accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### üßµ `builder.add_edge(...)`\n",
    "These are direct connections for each department to finish their work and then end the conversation:\n",
    "\n",
    "```python\n",
    "builder.add_edge(\"technical\", END)\n",
    "builder.add_edge(\"financial\", END)\n",
    "builder.add_edge(\"miscellaneous\", END)\n",
    "```\n",
    "- No more decision-making here. Once the task hits the right advisor node, it wraps up the session.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è `graph = builder.compile()`\n",
    "- This final step **compiles** everything.\n",
    "- Like clicking ‚ÄúBuild‚Äù after designing a workflow ‚Äî now it‚Äôs ready to run!\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ So what did you just create?\n",
    "A smart, dynamic workflow that:\n",
    "- Starts with a receptionist\n",
    "- Checks if the problem is solved, or needs help\n",
    "- Passes to supervisor if needed\n",
    "- Then sends to the right department\n",
    "- And closes out once help is given\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "XIAPfaxaHCCm",
    "outputId": "30006f80-94b9-4b6e-a7c2-37b5ef17ca5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Customer Support Flow ===\n",
      "Receptionist: Has your issue been resolved?\n",
      "User: no\n",
      "Supervisor decision: Technical\n",
      "\n",
      "üîß Technical Advisor: Helping with technical issue...\n"
     ]
    }
   ],
   "source": [
    "# Test Case\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Customer Support Flow ===\")\n",
    "    test_state = {\"issue\": \"My internet is not working properly.\"}\n",
    "    result = graph.invoke(test_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVBONNTiIEh0"
   },
   "outputs": [],
   "source": [
    "gkey=\"AIzaSyCOpoQvsNT6ylEd87-lY7-_b2YeiMfyaws\"\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = gkey\n",
    "\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Optional\n",
    "\n",
    "# Configure Gemini\n",
    "# Ensure GOOGLE_API_KEY is set as an environment variable\n",
    "try:\n",
    "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "    gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "except KeyError:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Please set the GOOGLE_API_KEY environment variable before running the script.\")\n",
    "    exit() # Exit if API key is not set\n",
    "\n",
    "# Define shared state type using TypedDict for better type hinting\n",
    "class SupportState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the customer support interaction.\n",
    "\n",
    "    Attributes:\n",
    "        issue (str): The initial issue described by the customer.\n",
    "        resolved (bool): True if the issue is considered resolved, False otherwise.\n",
    "        department (Optional[str]): The department the issue is routed to (technical, financial, miscellaneous).\n",
    "        history (List[AIMessage | HumanMessage]): A list of messages representing the conversation history.\n",
    "    \"\"\"\n",
    "    issue: str\n",
    "    resolved: bool\n",
    "    department: Optional[str]\n",
    "    history: List[AIMessage | HumanMessage]\n",
    "\n",
    "# Receptionist node\n",
    "def receptionist_node(state: SupportState) -> SupportState:\n",
    "    \"\"\"\n",
    "    The receptionist node initiates the conversation and checks if the issue is resolved.\n",
    "    If not, it escalates to the supervisor.\n",
    "    \"\"\"\n",
    "    user_input = state.get(\"issue\", \"\")\n",
    "    print(\"\\n--- Receptionist Phase ---\")\n",
    "\n",
    "    # The receptionist asks if the issue is resolved based on the initial input\n",
    "    llm_prompt = f\"The customer's initial issue is: '{user_input}'. Ask them if their issue has been resolved. Phrase it as a direct question.\"\n",
    "    response = gemini_model.generate_content(llm_prompt)\n",
    "    receptionist_question = response.text.strip()\n",
    "\n",
    "    print(f\"Receptionist: {receptionist_question}\")\n",
    "    state[\"history\"].append(AIMessage(content=f\"Receptionist: {receptionist_question}\"))\n",
    "\n",
    "    answer = input(\"User: \").lower()\n",
    "    state[\"history\"].append(HumanMessage(content=f\"User: {answer}\")) # Record user's answer\n",
    "\n",
    "    state[\"resolved\"] = \"yes\" in answer or \"y\" in answer\n",
    "    return state\n",
    "\n",
    "# Supervisor node\n",
    "def supervisor_node(state: SupportState) -> SupportState:\n",
    "    \"\"\"\n",
    "    The supervisor node analyzes the issue and routes it to the appropriate department.\n",
    "    \"\"\"\n",
    "    issue = state.get(\"issue\", \"\")\n",
    "    print(\"\\n--- Supervisor Phase ---\")\n",
    "\n",
    "    # LLM decides which department to route the issue to\n",
    "    llm_prompt = (\n",
    "        f\"The customer's issue is: '{issue}'. \"\n",
    "        f\"Based on this, route the issue to one of the following departments: \"\n",
    "        f\"'technical', 'financial', or 'miscellaneous'. \"\n",
    "        f\"Respond with only the department name, e.g., 'technical'.\"\n",
    "    )\n",
    "    response = gemini_model.generate_content(llm_prompt)\n",
    "    supervisor_decision_raw = response.text.strip().lower()\n",
    "\n",
    "    # Simple parsing of the LLM's decision\n",
    "    if \"technical\" in supervisor_decision_raw:\n",
    "        state[\"department\"] = \"technical\"\n",
    "    elif \"financial\" in supervisor_decision_raw:\n",
    "        state[\"department\"] = \"financial\"\n",
    "    else:\n",
    "        state[\"department\"] = \"miscellaneous\" # Default to miscellaneous\n",
    "\n",
    "    print(f\"Supervisor decision: Routing to {state['department']} department.\")\n",
    "    state[\"history\"].append(AIMessage(content=f\"Supervisor: Routing to {state['department']} department.\"))\n",
    "    return state\n",
    "\n",
    "# Advisor nodes (enhanced for interaction)\n",
    "def technical_advisor(state: SupportState) -> SupportState:\n",
    "    \"\"\"\n",
    "    The technical advisor node provides assistance for technical issues and loops\n",
    "    until the user confirms the issue is resolved.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Technical Advisor Phase ---\")\n",
    "    issue = state.get(\"issue\", \"\")\n",
    "    current_history = state.get(\"history\", [])\n",
    "\n",
    "    # Generate a response from the technical advisor based on the issue and history\n",
    "    prompt = (\n",
    "        f\"You are a technical support advisor. The customer's issue is: '{issue}'. \"\n",
    "        f\"Based on the current conversation history: {current_history}, \"\n",
    "        f\"provide a helpful technical tip, ask a clarifying question, or suggest a troubleshooting step. \"\n",
    "        f\"Keep your response concise and directly address the technical problem.\"\n",
    "    )\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "    advisor_message = response.text.strip()\n",
    "\n",
    "    print(f\"üîß Technical Advisor: {advisor_message}\")\n",
    "    state[\"history\"].append(AIMessage(content=f\"Technical Advisor: {advisor_message}\"))\n",
    "\n",
    "    # Ask if resolved and get user input\n",
    "    print(\"Technical Advisor: Has your issue been resolved? (yes/no)\")\n",
    "    answer = input(\"User: \").lower()\n",
    "    state[\"history\"].append(HumanMessage(content=f\"User: {answer}\")) # Record user's answer\n",
    "\n",
    "    state[\"resolved\"] = \"yes\" in answer or \"y\" in answer\n",
    "    return state\n",
    "\n",
    "def financial_advisor(state: SupportState) -> SupportState:\n",
    "    \"\"\"\n",
    "    The financial advisor node provides assistance for financial issues and loops\n",
    "    until the user confirms the issue is resolved.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Financial Advisor Phase ---\")\n",
    "    issue = state.get(\"issue\", \"\")\n",
    "    current_history = state.get(\"history\", [])\n",
    "\n",
    "    # Generate a response from the financial advisor\n",
    "    prompt = (\n",
    "        f\"You are a financial support advisor. The customer's issue is: '{issue}'. \"\n",
    "        f\"Based on the current conversation history: {current_history}, \"\n",
    "        f\"provide a helpful financial tip, ask a clarifying question, or suggest a step related to billing/payments. \"\n",
    "        f\"Keep your response concise and directly address the financial problem.\"\n",
    "    )\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "    advisor_message = response.text.strip()\n",
    "\n",
    "    print(f\"üí∞ Financial Advisor: {advisor_message}\")\n",
    "    state[\"history\"].append(AIMessage(content=f\"Financial Advisor: {advisor_message}\"))\n",
    "\n",
    "    # Ask if resolved and get user input\n",
    "    print(\"Financial Advisor: Has your issue been resolved? (yes/no)\")\n",
    "    answer = input(\"User: \").lower()\n",
    "    state[\"history\"].append(HumanMessage(content=f\"User: {answer}\")) # Record user's answer\n",
    "\n",
    "    state[\"resolved\"] = \"yes\" in answer or \"y\" in answer\n",
    "    return state\n",
    "\n",
    "def misc_advisor(state: SupportState) -> SupportState:\n",
    "    \"\"\"\n",
    "    The miscellaneous advisor node handles general questions and loops\n",
    "    until the user confirms the issue is resolved.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Miscellaneous Advisor Phase ---\")\n",
    "    issue = state.get(\"issue\", \"\")\n",
    "    current_history = state.get(\"history\", [])\n",
    "\n",
    "    # Generate a response from the miscellaneous advisor\n",
    "    prompt = (\n",
    "        f\"You are a general support advisor. The customer's issue is: '{issue}'. \"\n",
    "        f\"Based on the current conversation history: {current_history}, \"\n",
    "        f\"provide a helpful general tip, ask a clarifying question, or suggest a next step. \"\n",
    "        f\"Keep your response concise and directly address the general problem.\"\n",
    "    )\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "    advisor_message = response.text.strip()\n",
    "\n",
    "    print(f\"üß≠ Miscellaneous Advisor: {advisor_message}\")\n",
    "    state[\"history\"].append(AIMessage(content=f\"Miscellaneous Advisor: {advisor_message}\"))\n",
    "\n",
    "    # Ask if resolved and get user input\n",
    "    print(\"Miscellaneous Advisor: Has your issue been resolved? (yes/no)\")\n",
    "    answer = input(\"User: \").lower()\n",
    "    state[\"history\"].append(HumanMessage(content=f\"User: {answer}\")) # Record user's answer\n",
    "\n",
    "    state[\"resolved\"] = \"yes\" in answer or \"y\" in answer\n",
    "    return state\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(SupportState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"receptionist\", receptionist_node)\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"technical\", technical_advisor)\n",
    "builder.add_node(\"financial\", financial_advisor)\n",
    "builder.add_node(\"miscellaneous\", misc_advisor)\n",
    "\n",
    "# Define entry point\n",
    "builder.set_entry_point(\"receptionist\")\n",
    "\n",
    "# Define edges\n",
    "# From receptionist: if resolved, end; otherwise, go to supervisor\n",
    "builder.add_conditional_edges(\n",
    "    \"receptionist\",\n",
    "    lambda state: END if state[\"resolved\"] else \"supervisor\"\n",
    ")\n",
    "\n",
    "# From supervisor: route to the determined department\n",
    "builder.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda state: state[\"department\"]\n",
    ")\n",
    "\n",
    "# From advisor nodes: if resolved, end; otherwise, loop back to the same advisor\n",
    "builder.add_conditional_edges(\n",
    "    \"technical\",\n",
    "    lambda state: END if state[\"resolved\"] else \"technical\"\n",
    ")\n",
    "builder.add_conditional_edges(\n",
    "    \"financial\",\n",
    "    lambda state: END if state[\"resolved\"] else \"financial\"\n",
    ")\n",
    "builder.add_conditional_edges(\n",
    "    \"miscellaneous\",\n",
    "    lambda state: END if state[\"resolved\"] else \"miscellaneous\"\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "graph = builder.compile()\n",
    "\n",
    "# Test Case\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Customer Support Flow Simulation ===\")\n",
    "    initial_state = SupportState(\n",
    "        issue=\"My internet is not working properly, I can't connect to any websites.\",\n",
    "        resolved=False,\n",
    "        department=None,\n",
    "        history=[HumanMessage(content=\"Customer: My internet is not working properly, I can't connect to any websites.\")]\n",
    "    )\n",
    "    result = graph.invoke(initial_state)\n",
    "    print(\"\\n=== Flow Ended ===\")\n",
    "    print(\"Final State:\")\n",
    "    print(result)\n",
    "    print(\"\\nConversation History:\")\n",
    "    for msg in result[\"history\"]:\n",
    "        print(f\"{msg.type}: {msg.content}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\r\n",
    "\r\n",
    "### Understanding the Control Flow with `StateGraph`\r\n",
    "\r\n",
    "The core of this system is `langgraph`'s `StateGraph`, which orchestrates a series of \"nodes\" (your functions like `receptionist_node`, `supervisor_node`, etc.) based on a shared state.\r\n",
    "\r\n",
    "1.  **Shared State (`SupportState`):**\r\n",
    "    * Think of `SupportState` as a central blackboard where all nodes can read and write information. It holds critical details like the `issue`, whether it's `resolved`, the `department` it's routed to, and the `history` of the conversation.\r\n",
    "    * Each node receives the current `SupportState`, performs its logic, and then **returns an updated `SupportState`**. This updated state is then passed to the next node in the graph.\r\n",
    "\r\n",
    "2.  **Nodes as Functions:**\r\n",
    "    * Each function (`receptionist_node`, `supervisor_node`, `technical_advisor`, etc.) represents a specific step or agent in your customer support process.\r\n",
    "    * **Why `return state`?** When a node finishes its work, it needs to hand over the *modified* state to the graph. By returning `state`, you're essentially saying, \"Here's the current snapshot of our conversation and its status after my part is done. Pass this on to whoever is next.\" If you didn't return the state, the changes made within the node would be lost, and the next node would receive an outdated state.\r\n",
    "\r\n",
    "3.  **Edges and Conditional Transitions:**\r\n",
    "    * **Edges** define the paths between nodes. They dictate which node comes next.\r\n",
    "    * `builder.add_node(\"receptionist\", receptionist_node)`: This simply registers your Python function `receptionist_node` with the name \"receptionist\" in the graph.\r\n",
    "    * `builder.set_entry_point(\"receptionist\")`: This tells the graph where to start when `graph.invoke()` is called.\r\n",
    "\r\n",
    "    * **`add_conditional_edges` and `lambda`:** This is where the dynamic routing happens, and `lambda` functions are key.\r\n",
    "        * `builder.add_conditional_edges(\"receptionist\", lambda state: END if state[\"resolved\"] else \"supervisor\")`\r\n",
    "        * When the \"receptionist\" node finishes, `langgraph` calls the `lambda` function provided here.\r\n",
    "        * The **`lambda state: ...`** is a small, anonymous function that takes the `state` (the `SupportState` returned by `receptionist_node`) as input.\r\n",
    "        * It then evaluates the condition: `state[\"resolved\"]`.\r\n",
    "            * If `state[\"resolved\"]` is `True`, the `lambda` function returns `END`, signifying the end of the graph execution.\r\n",
    "            * If `state[\"resolved\"]` is `False`, the `lambda` function returns `\"supervisor\"`, instructing the graph to transition to the \"supervisor\" node next.\r\n",
    "\r\n",
    "        * Similarly, for the `supervisor` node:\r\n",
    "            * `builder.add_conditional_edges(\"supervisor\", lambda state: state[\"department\"])`\r\n",
    "            * After the `supervisor_node` updates the `state` with the `department`, this `lambda` function simply returns the value of `state[\"department\"]` (e.g., \"technical\", \"financial\", or \"miscellaneous\"). `langgraph` then uses this returned string to find and execute the corresponding node (e.g., \"technical\" routes to `technical_advisor`).\r\n",
    "\r\n",
    "        * And for the advisor nodes:\r\n",
    "            * `builder.add_conditional_edges(\"technical\", lambda state: END if state[\"resolved\"] else \"technical\")`\r\n",
    "            * After the `technical_advisor` node executes, this `lambda` checks `state[\"resolved\"]`. If `True`, the flow ends. If `False`, it returns `\"technical\"`, effectively creating a **loop** where the flow goes back to the `technical_advisor` node again for further interaction until the issue is resolved.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Step-by-Step Flow for the Test Case\r\n",
    "\r\n",
    "Let's trace your example: \"My internet is not working properly, I can't connect to any websites.\"\r\n",
    "\r\n",
    "1.  **`graph.invoke(initial_state)`:**\r\n",
    "    * The graph starts at the `receptionist` node.\r\n",
    "\r\n",
    "2.  **`receptionist_node` execution:**\r\n",
    "    * Receives `initial_state`.\r\n",
    "    * Prints \"--- Receptionist Phase ---\".\r\n",
    "    * LLM generates: \"Has your internet issue been resolved?\"\r\n",
    "    * You, as the user, input \"no\".\r\n",
    "    * `state[\"resolved\"]` is set to `False`.\r\n",
    "    * `state[\"history\"]` is updated.\r\n",
    "    * `receptionist_node` **returns** the updated `state`.\r\n",
    "\r\n",
    "3.  **Receptionist's Conditional Edge:**\r\n",
    "    * `lambda state: END if state[\"resolved\"] else \"supervisor\"` is evaluated.\r\n",
    "    * Since `state[\"resolved\"]` is `False`, the lambda returns `\"supervisor\"`.\r\n",
    "    * The graph transitions to the `supervisor` node.\r\n",
    "\r\n",
    "4.  **`supervisor_node` execution:**\r\n",
    "    * Receives the state (with `resolved: False`, updated history).\r\n",
    "    * Prints \"--- Supervisor Phase ---\".\r\n",
    "    * LLM analyzes \"My internet is not working properly...\" and determines \"technical\".\r\n",
    "    * `state[\"department\"]` is set to \"technical\".\r\n",
    "    * `state[\"history\"]` is updated.\r\n",
    "    * `supervisor_node` **returns** the updated `state`.\r\n",
    "\r\n",
    "5.  **Supervisor's Conditional Edge:**\r\n",
    "    * `lambda state: state[\"department\"]` is evaluated.\r\n",
    "    * Since `state[\"department\"]` is \"technical\", the lambda returns `\"technical\"`.\r\n",
    "    * The graph transitions to the `technical` node (`technical_advisor` function).\r\n",
    "\r\n",
    "6.  **`technical_advisor` execution (First pass):**\r\n",
    "    * Receives the state (with `department: \"technical\"`, updated history).\r\n",
    "    * Prints \"--- Technical Advisor Phase ---\".\r\n",
    "    * LLM generates a technical tip (e.g., \"Have you tried restarting your router?\").\r\n",
    "    * You, as the user, input \"no\" (assuming the issue isn't fixed yet).\r\n",
    "    * `state[\"resolved\"]` is still `False`.\r\n",
    "    * `state[\"history\"]` is updated.\r\n",
    "    * `technical_advisor` **returns** the updated `state`.\r\n",
    "\r\n",
    "7.  **Technical Advisor's Conditional Edge:**\r\n",
    "    * `lambda state: END if state[\"resolved\"] else \"technical\"` is evaluated.\r\n",
    "    * Since `state[\"resolved\"]` is `False`, the lambda returns `\"technical\"`.\r\n",
    "    * The graph **loops back** to the `technical` node (`technical_advisor` function).\r\n",
    "\r\n",
    "8.  **`technical_advisor` execution (Second pass - or more, until resolved):**\r\n",
    "    * The process repeats. The advisor continues to provide tips or ask questions.\r\n",
    "    * Let's say in a subsequent interaction, you input \"yes\" because the issue is finally resolved.\r\n",
    "    * `state[\"resolved\"]` is set to `True`.\r\n",
    "    * `technical_advisor` **returns** the updated `state`.\r\n",
    "\r\n",
    "9.  **Technical Advisor's Conditional Edge (Again):**\r\n",
    "    * `lambda state: END if state[\"resolved\"] else \"technical\"` is evaluated.\r\n",
    "    * Now, `state[\"resolved\"]` is `True`, so the lambda returns `END`.\r\n",
    "    * The graph execution stops.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### In Summary\r\n",
    "\r\n",
    "* **`return state` in functions:** This is how each node passes its changes and the current \"status\" of the conversation to the next step in the `StateGraph`. Without it, the state wouldn't persist or update correctly.\r\n",
    "* **`lambda` functions in `add_conditional_edges`:** These are lightweight, inline functions that `langgraph` executes *after* a node finishes. They use the `state` returned by the just-completed node to dynamically decide which path (which next node, or `END`) the graph should take. They are powerful for implementing conditional logic and loops in your graph.\r\n",
    "\r\n",
    "This setup allows for a flexible and dynamic conversation flow\n",
    "Does this clarification help in understanding the control flow and the roles of `return state` and `lambda`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the updated code, replacing the `lambda` function for the \"supervisor\" node's conditional edge with a named function using an `if` statement (though a direct return is still the most concise way for this specific logic).\r\n",
    "\r\n",
    "I've defined a new function `route_supervisor_decision` that explicitly returns the department based on the state.\r\n",
    "\r\n",
    "```python\r\n",
    "import os\r\n",
    "import google.generativeai as genai\r\n",
    "from langchain_core.messages import HumanMessage, AIMessage\r\n",
    "from langgraph.graph import StateGraph, END\r\n",
    "from typing import TypedDict, List, Optional\r\n",
    "\r\n",
    "# Configure Gemini\r\n",
    "# Ensure GOOGLE_API_KEY is set as an environment variable\r\n",
    "try:\r\n",
    "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\r\n",
    "    gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\r\n",
    "except KeyError:\r\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\r\n",
    "    print(\"Please set the GOOGLE_API_KEY environment variable before running the script.\")\r\n",
    "    exit() # Exit if API key is not set\r\n",
    "\r\n",
    "# Define shared state type using TypedDict for better type hinting\r\n",
    "class SupportState(TypedDict):\r\n",
    "    \"\"\"\r\n",
    "    Represents the state of the customer support interaction.\r\n",
    "\r\n",
    "    Attributes:\r\n",
    "        issue (str): The initial issue described by the customer.\r\n",
    "        resolved (bool): True if the issue is considered resolved, False otherwise.\r\n",
    "        department (Optional[str]): The department the issue is routed to (technical, financial, miscellaneous).\r\n",
    "        history (List[AIMessage | HumanMessage]): A list of messages representing the conversation history.\r\n",
    "    \"\"\"\r\n",
    "    issue: str\r\n",
    "    resolved: bool\r\n",
    "    department: Optional[str]\r\n",
    "    history: List[AIMessage | HumanMessage]\r\n",
    "\r\n",
    "# Receptionist node\r\n",
    "def receptionist_node(state: SupportState) -> SupportState:\r\n",
    "    \"\"\"\r\n",
    "    The receptionist node initiates the conversation and checks if the issue is resolved.\r\n",
    "    If not, it escalates to the supervisor.\r\n",
    "    \"\"\"\r\n",
    "    user_input = state.get(\"issue\", \"\")\r\n",
    "    print(\"\\n--- Receptionist Phase ---\")\r\n",
    "\r\n",
    "    # The receptionist asks if the issue is resolved based on the initial input\r\n",
    "    llm_prompt = f\"The customer's initial issue is: '{user_input}'. Ask them if their issue has been resolved. Phrase it as a direct question.\"\r\n",
    "    response = gemini_model.generate_content(llm_prompt)\r\n",
    "    receptionist_question = response.text.strip()\r\n",
    "\r\n",
    "    print(f\"Receptionist: {receptionist_question}\")\r\n",
    "    state[\"history\"].append(AIMessage(content=f\"Receptionist: {receptionist_question}\"))\r\n",
    "\r\n",
    "    answer = input(\"User: \").lower()\r\n",
    "    state[\"history\"].append(HumanMessage(content=f\"User: {answer}\")) # Record user's answer\r\n",
    "\r\n",
    "    state[\"resolved\"] = \"yes\" in answer or \"y\" in answer\r\n",
    "    return state\r\n",
    "\r\n",
    "# Supervisor node\r\n",
    "def supervisor_node(state: SupportState) -> SupportState:\r\n",
    "    \"\"\"\r\n",
    "    The supervisor node analyzes the issue and routes it to the appropriate department.\r\n",
    "    \"\"\"\r\n",
    "    issue = state.get(\"issue\", \"\")\r\n",
    "    print(\"\\n--- Supervisor Phase ---\")\r\n",
    "\r\n",
    "    # LLM decides which department to route the issue to\r\n",
    "    llm_prompt = (\r\n",
    "        f\"The customer's issue is: '{issue}'. \"\r\n",
    "        f\"Based on this, route the issue to one of the following departments: \"\r\n",
    "        f\"'technical', 'financial', or 'miscellaneous'. \"\r\n",
    "        f\"Respond with only the department name, e.g., 'technical'.\"\r\n",
    "    )\r\n",
    "    response = gemini_model.generate_content(llm_prompt)\r\n",
    "    supervisor_decision_raw = response.text.strip().lower()\r\n",
    "\r\n",
    "    # Simple parsing of the LLM's decision\r\n",
    "    if \"technical\" in supervisor_decision_raw:\r\n",
    "        state[\"department\"] = \"technical\"\r\n",
    "    elif \"financial\" in supervisor_decision_raw:\r\n",
    "        state[\"department\"] = \"financial\"\r\n",
    "    else:\r\n",
    "        state[\"department\"] = \"miscellaneous\" # Default to miscellaneous\r\n",
    "\r\n",
    "    print(f\"Supervisor decision: Routing to {state['department']} department.\")\r\n",
    "    state[\"history\"].append(AIMessage(content=f\"Supervisor: Routing to {state['department']} department.\"))\r\n",
    "    return state\r\n",
    "\r\n",
    "# Advisor nodes (enhanced for interaction)\r\n",
    "def technical_advisor(state: SupportState) -> SupportState:\r\n",
    "    \"\"\"\r\n",
    "    The technical advisor node provides assistance for technical issues and loops\r\n",
    "    until the user confirms the issue is resolved.\r\n",
    "    \"\"\"\r\n",
    "    print(\"\\n--- Technical Advisor Phase ---\")\r\n",
    "    issue = state.get(\"issue\", \"\")\r\n",
    "    current_history = state.get(\"history\", [])\r\n",
    "\r\n",
    "    # Generate a response from the technical advisor based on the issue and history\r\n",
    "    prompt = (\r\n",
    "        f\"You are a technical support advisor. The customer's issue is: '{issue}'. \"\r\n",
    "        f\"Based on the current conversation history: {current_history}, \"\r\n",
    "        f\"provide a helpful technical tip, ask a clarifying question, or suggest a troubleshooting step. \"\r\n",
    "        f\"Keep your response concise and directly address the technical problem.\"\r\n",
    "    )\r\n",
    "    response = gemini_model.generate_content(prompt)\r\n",
    "    advisor_message = response.text.strip()\r\n",
    "\r\n",
    "    print(f\"üîß Technical Advisor: {advisor_message}\")\r\n",
    "    state[\"history\"].append(AIMessage(content=f\"Technical Advisor: {advisor_message}\"))\r\n",
    "\r\n",
    "    # Ask if resolved and get user input\r\n",
    "    print(\"Technical Advisor: Has your issue been resolved? (yes/no)\")\r\n",
    "    answer = input(\"User: \").lower()\r\n",
    "    state[\"history\"].append(HumanMessage(content=f\"User: {answer}\")) # Record user's answer\r\n",
    "\r\n",
    "    state[\"resolved\"] = \"yes\" in answer or \"y\" in answer\r\n",
    "    return state\r\n",
    "\r\n",
    "def financial_advisor(state: SupportState) -> SupportState:\r\n",
    "    \"\"\"\r\n",
    "    The financial advisor node provides assistance for financial issues and loops\r\n",
    "    until the user confirms the issue is resolved.\r\n",
    "    \"\"\"\r\n",
    "    print(\"\\n--- Financial Advisor Phase ---\")\r\n",
    "    issue = state.get(\"issue\", \"\")\r\n",
    "    current_history = state.get(\"history\", [])\r\n",
    "\r\n",
    "    # Generate a response from the financial advisor\r\n",
    "    prompt = (\r\n",
    "        f\"You are a financial support advisor. The customer's issue is: '{issue}'. \"\r\n",
    "        f\"Based on the current conversation history: {current_history}, \"\r\n",
    "        f\"provide a helpful financial tip, ask a clarifying question, or suggest a step related to billing/payments. \"\r\n",
    "        f\"Keep your response concise and directly address the financial problem.\"\r\n",
    "    )\r\n",
    "    response = gemini_model.generate_content(prompt)\r\n",
    "    advisor_message = response.text.strip()\r\n",
    "\r\n",
    "    print(f\"üí∞ Financial Advisor: {advisor_message}\")\r\n",
    "    state[\"history\"].append(AIMessage(content=f\"Financial Advisor: {advisor_message}\"))\r\n",
    "\r\n",
    "    # Ask if resolved and get user input\r\n",
    "    print(\"Financial Advisor: Has your issue been resolved? (yes/no)\")\r\n",
    "    answer = input(\"User: \").lower()\r\n",
    "    state[\"history\"].append(HumanMessage(content=f\"User: {answer}\")) # Record user's answer\r\n",
    "\r\n",
    "    state[\"resolved\"] = \"yes\" in answer or \"y\" in answer\r\n",
    "    return state\r\n",
    "\r\n",
    "def misc_advisor(state: SupportState) -> SupportState:\r\n",
    "    \"\"\"\r\n",
    "    The miscellaneous advisor node handles general questions and loops\r\n",
    "    until the user confirms the issue is resolved.\r\n",
    "    \"\"\"\r\n",
    "    print(\"\\n--- Miscellaneous Advisor Phase ---\")\r\n",
    "    issue = state.get(\"issue\", \"\")\r\n",
    "    current_history = state.get(\"history\", [])\r\n",
    "\r\n",
    "    # Generate a response from the miscellaneous advisor\r\n",
    "    prompt = (\r\n",
    "        f\"You are a general support advisor. The customer's issue is: '{issue}'. \"\r\n",
    "        f\"Based on the current conversation history: {current_history}, \"\r\n",
    "        f\"provide a helpful general tip, ask a clarifying question, or suggest a next step. \"\r\n",
    "        f\"Keep your response concise and directly address the general problem.\"\r\n",
    "    )\r\n",
    "    response = gemini_model.generate_content(prompt)\r\n",
    "    advisor_message = response.text.strip()\r\n",
    "\r\n",
    "    print(f\"üß≠ Miscellaneous Advisor: {advisor_message}\")\r\n",
    "    state[\"history\"].append(AIMessage(content=f\"Miscellaneous Advisor: {advisor_message}\"))\r\n",
    "\r\n",
    "    # Ask if resolved and get user input\r\n",
    "    print(\"Miscellaneous Advisor: Has your issue been resolved? (yes/no)\")\r\n",
    "    answer = input(\"User: \").lower()\r\n",
    "    state[\"history\"].append(HumanMessage(content=f\"User: {answer}\")) # Record user's answer\r\n",
    "\r\n",
    "    state[\"resolved\"] = \"yes\" in answer or \"y\" in answer\r\n",
    "    return state\r\n",
    "\r\n",
    "# NEW: Function to route based on supervisor's decision\r\n",
    "def route_supervisor_decision(state: SupportState) -> str:\r\n",
    "    \"\"\"\r\n",
    "    Determines the next department based on the supervisor's routing decision.\r\n",
    "    \"\"\"\r\n",
    "    # In this specific case, the supervisor node already sets the 'department'\r\n",
    "    # so we just need to return that value.\r\n",
    "    if state[\"department\"] == \"technical\":\r\n",
    "        return \"technical\"\r\n",
    "    elif state[\"department\"] == \"financial\":\r\n",
    "        return \"financial\"\r\n",
    "    elif state[\"department\"] == \"miscellaneous\":\r\n",
    "        return \"miscellaneous\"\r\n",
    "    else:\r\n",
    "        # Fallback in case department is unexpectedly None or an invalid value\r\n",
    "        return \"miscellaneous\"\r\n",
    "\r\n",
    "# Build the graph\r\n",
    "builder = StateGraph(SupportState)\r\n",
    "\r\n",
    "# Add nodes\r\n",
    "builder.add_node(\"receptionist\", receptionist_node)\r\n",
    "builder.add_node(\"supervisor\", supervisor_node)\r\n",
    "builder.add_node(\"technical\", technical_advisor)\r\n",
    "builder.add_node(\"financial\", financial_advisor)\r\n",
    "builder.add_node(\"miscellaneous\", misc_advisor)\r\n",
    "\r\n",
    "# Define entry point\r\n",
    "builder.set_entry_point(\"receptionist\")\r\n",
    "\r\n",
    "# Define edges\r\n",
    "# From receptionist: if resolved, end; otherwise, go to supervisor\r\n",
    "builder.add_conditional_edges(\r\n",
    "    \"receptionist\",\r\n",
    "    lambda state: END if state[\"resolved\"] else \"supervisor\"\r\n",
    ")\r\n",
    "\r\n",
    "# From supervisor: route to the determined department using the new function\r\n",
    "builder.add_conditional_edges(\r\n",
    "    \"supervisor\",\r\n",
    "    route_supervisor_decision # Using the named function here\r\n",
    ")\r\n",
    "\r\n",
    "# From advisor nodes: if resolved, end; otherwise, loop back to the same advisor\r\n",
    "builder.add_conditional_edges(\r\n",
    "    \"technical\",\r\n",
    "    lambda state: END if state[\"resolved\"] else \"technical\"\r\n",
    ")\r\n",
    "builder.add_conditional_edges(\r\n",
    "    \"financial\",\r\n",
    "    lambda state: END if state[\"resolved\"] else \"financial\"\r\n",
    ")\r\n",
    "builder.add_conditional_edges(\r\n",
    "    \"miscellaneous\",\r\n",
    "    lambda state: END if state[\"resolved\"] else \"miscellaneous\"\r\n",
    ")\r\n",
    "\r\n",
    "# Compile the graph\r\n",
    "graph = builder.compile()\r\n",
    "\r\n",
    "# Test Case\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    print(\"=== Customer Support Flow Simulation ===\")\r\n",
    "    initial_state = SupportState(\r\n",
    "        issue=\"My internet is not working properly, I can't connect to any websites.\",\r\n",
    "        resolved=False,\r\n",
    "        department=None,\r\n",
    "        history=[HumanMessage(content=\"Customer: My internet is not working properly, I can't connect to any websites.\")]\r\n",
    "    )\r\n",
    "    result = graph.invoke(initial_state)\r\n",
    "    print(\"\\n=== Flow Ended ===\")\r\n",
    "    print(\"Final State:\")\r\n",
    "    print(result)\r\n",
    "    print(\"\\nConversation History:\")\r\n",
    "    for msg in result[\"history\"]:\r\n",
    "        print(f\"{msg.type}: {msg.content}\")\r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
